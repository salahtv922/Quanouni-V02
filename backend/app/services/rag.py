import requests
import json
import re
import time
from dataclasses import dataclass
from typing import Optional
from app.core.config import settings
from app.services.embedding import get_embedding
from app.services.vector_store import query_chroma


@dataclass
class GenerationResponse:
    text: str


def generate_with_retry(model, prompt, retries=5, delay=4):
    # Check if Groq is enabled (Preferred for Generation)
    if hasattr(settings, 'GROQ_API_KEY') and settings.GROQ_API_KEY:
        try:
            # Groq API Call
            headers = {
                "Authorization": f"Bearer {settings.GROQ_API_KEY}",
                "Content-Type": "application/json"
            }
            # Fallback to stable model if the configured one fails
            model_id = getattr(settings, 'GROQ_MODEL', None) or "llama-3.1-70b-versatile"
            print(f"[Groq] Using model: {model_id}")
            data = {
                "messages": [{"role": "user", "content": prompt}],
                "model": model_id,
                "temperature": 0.3,
                "max_tokens": 4096
            }
            
            # Simple retry logic for Groq too
            for attempt in range(3):
                try:
                    resp = requests.post("https://api.groq.com/openai/v1/chat/completions", headers=headers, json=data, timeout=120)
                    if resp.status_code == 200:
                        content = resp.json()['choices'][0]['message']['content']
                        return GenerationResponse(text=content)
                    elif resp.status_code == 429:
                        print(f"Groq Rate Limit, waiting {delay}s...")
                        time.sleep(delay)
                        continue
                    else:
                        print(f"Groq Error {resp.status_code}: {resp.text[:500]}")
                except requests.exceptions.Timeout:
                    print(f"Groq Timeout (attempt {attempt+1}/3)")
                    continue
                except Exception as e:
                    print(f"Groq Exception: {type(e).__name__}: {e}")
            
            print("Groq failed, falling back to Gemini...")
        except Exception as e:
             print(f"Groq Setup Error: {e}, falling back...")

    # Fallback to Gemini REST API
    url = f"https://generativelanguage.googleapis.com/v1beta/models/{settings.GEMINI_CHAT_MODEL}:generateContent?key={settings.GEMINI_API_KEY}"
    
    headers = {"Content-Type": "application/json"}
    payload = {
        "contents": [{"parts": [{"text": prompt}]}],
        "generationConfig": {
            "temperature": 0.7,
            "maxOutputTokens": 8192
        }
    }

    for attempt in range(retries):
        try:
            resp = requests.post(url, headers=headers, json=payload, timeout=60)
            if resp.status_code == 200:
                result = resp.json()
                # Extract text from Gemini response structure
                try:
                    text = result['candidates'][0]['content']['parts'][0]['text']
                    return GenerationResponse(text=text)
                except (KeyError, IndexError):
                     print(f"Gemini Bad Response format: {result}")
                     raise ValueError("Gemini response parsing failed")
            elif resp.status_code == 429:
                 print(f"Gemini Rate Limit, waiting {delay}s...")
                 time.sleep(delay)
                 delay *= 2
            else:
                 print(f"Gemini Error {resp.status_code}: {resp.text}")
                 if attempt == retries - 1:
                     raise Exception(f"Gemini API Error: {resp.text}")
                 
        except Exception as e:
            if attempt < retries - 1:
                print(f"Gemini Exception: {e}, retrying...")
                time.sleep(delay)
            else:
                raise e

def detect_language(text: str) -> str:
    """Detect the language of the query"""
    arabic_chars = sum(1 for c in text if '\u0600' <= c <= '\u06FF')
    total_chars = len([c for c in text if c.isalpha()])
    
    if total_chars == 0:
        return "ar"
    
    arabic_ratio = arabic_chars / total_chars
    
    if arabic_ratio > 0.3:
        return "ar"
    
    french_words = ['le', 'la', 'les', 'de', 'et', 'dans', 'pour', 'sont', 'peut', 'comment', 'quels', 'quel']
    text_lower = text.lower()
    if any(word in text_lower.split() for word in french_words):
        return "fr"
    
    return "en"

def rerank_with_gemini(query: str, chunks: list[str], top_k: int = 3) -> list[tuple[str, float]]:
    # Removed configure_gemini() call
    # model = genai.GenerativeModel(...) -> Just pass None as we use REST inside generate_with_retry
    model = None 

    
    chunks_text = ""
    for i, chunk in enumerate(chunks[:10], 1):
        chunks_text += f"\n\n### Chunk {i}:\n{chunk[:500]}...\n"
    
    prompt = f"""Ù‚ÙŠÙ‘Ù… Ù…Ø¯Ù‰ ØµÙ„Ø© ÙƒÙ„ chunk Ø¨Ø§Ù„Ø³Ø¤Ø§Ù„ Ø§Ù„ØªØ§Ù„ÙŠ. Ø£Ø¹Ø· Ø¯Ø±Ø¬Ø© Ù…Ù† 0 Ø¥Ù„Ù‰ 10 Ù„ÙƒÙ„ chunk.
Ø§Ù„Ø³Ø¤Ø§Ù„: {query}
Chunks: {chunks_text}
Ø§Ù„ØªØ¹Ù„ÙŠÙ…Ø§Øª:
- 10: Ø¥Ø¬Ø§Ø¨Ø© Ù…Ø¨Ø§Ø´Ø±Ø©
- 5-9: ØµÙ„Ø© Ø¬Ø²Ø¦ÙŠØ©
- 0-4: ØºÙŠØ± Ø°ÙŠ ØµÙ„Ø©
Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© JSON ÙÙ‚Ø·: {{"1": 8, ...}}"""
    
    try:
        response = generate_with_retry(model, prompt)
        import json
        json_match = re.search(r'\{[^}]+\}', response.text)
        if json_match:
            scores = json.loads(json_match.group())
            ranked = []
            for i, chunk in enumerate(chunks[:10], 1):
                score = float(scores.get(str(i), 0)) / 10.0
                ranked.append((chunk, score))
            for chunk in chunks[10:]:
                ranked.append((chunk, 0.1))
            ranked.sort(key=lambda x: x[1], reverse=True)
            return ranked[:top_k]
        return [(chunk, 0.5) for chunk in chunks[:top_k]]
    except Exception as e:
        # Avoid printing full exception if it contains Arabic
        return [(chunk, 0.5) for chunk in chunks[:top_k]]

class RAGService:
    def __init__(self):
        # No SDK configuration needed.
        self.model = None # We don't use the SDK model object anymore

    def _retrieve(self, query, filters=None, top_k=20):
        # 1. Vector Search
        try:
            query_embedding = get_embedding(query, is_query=True)
            vector_results = query_chroma(query_embedding, n_results=top_k, where=filters)
            v_docs = vector_results['documents'][0] if vector_results and 'documents' in vector_results else []
            v_metas = vector_results['metadatas'][0] if vector_results and 'metadatas' in vector_results else []
        except Exception as e:
            print(f"Vector search failed")
            v_docs, v_metas = [], []

        # 2. BM25 Search
        from app.services.bm25_service import bm25_service
        bm25_results = bm25_service.search(query, top_k=top_k, filters=filters)

        # 3. RRF Fusion
        k = 60
        scores = {}
        meta_map = {}
        
        # Combine (balanced weights for semantic + keyword search)
        for r, d in enumerate(v_docs):
            scores[d] = scores.get(d, 0) + (0.5 / (k + r + 1))  # Vector: 50%
            if r < len(v_metas): meta_map[d] = v_metas[r]
            
        for r, (d, s, m) in enumerate(bm25_results):
            scores[d] = scores.get(d, 0) + (0.5 / (k + r + 1))  # BM25: 50%
            meta_map[d] = m

        ranked_docs = sorted(scores.items(), key=lambda x: x[1], reverse=True)
        final_docs = [d for d, s in ranked_docs[:15]]
        final_metas = [meta_map.get(d, {}) for d in final_docs]
        
        return final_docs, final_metas

    def answer_query(self, query: str, filters: dict = None, skip_generation: bool = False):
        # Standard Research Mode
        docs, metas = self._retrieve(query, filters)
        
        # Rerank
        if not skip_generation:
            reranked = rerank_with_gemini(query, docs, top_k=5)
            # Re-map metadata
            final_docs = [r[0] for r in reranked]
            final_metas = []
            for d in final_docs:
                # Find meta again (inefficient but safe)
                try: 
                    idx = docs.index(d)
                    final_metas.append(metas[idx])
                except: final_metas.append({})
        else:
            final_docs, final_metas = docs[:5], metas[:5]

        # Context formatting
        context = ""
        for i, (doc, meta) in enumerate(zip(final_docs, final_metas), 1):
            title = meta.get('filename', f'Source {i}').replace('.txt', '')
            context += f"\n\n### [Ù…ØµØ¯Ø± {i}: {title}]\n{doc}\n"

        if skip_generation:
            return {"answer": "Retrieval Only", "context": final_docs, "metadatas": final_metas}

        # Prompt - Professional Legal Research (v2.0)
        prompt = f"""Ø£Ù†Øª **Ø¨Ø§Ø­Ø« Ù‚Ø§Ù†ÙˆÙ†ÙŠ Ù…ØªØ®ØµØµ ÙÙŠ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ† Ø§Ù„Ø¬Ø²Ø§Ø¦Ø±ÙŠ**ØŒ ØªØ¹Ù…Ù„ ÙÙŠ Ù…ÙƒØªØ¨Ø© Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© Ø£ÙƒØ§Ø¯ÙŠÙ…ÙŠØ©.
Ù…Ø±Ø¬Ø¹ÙŠØªÙƒ Ø§Ù„Ø­ØµØ±ÙŠØ© Ù‡ÙŠ Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© Ø§Ù„Ù…Ù‚Ø¯Ù…Ø© Ø£Ø¯Ù†Ø§Ù‡ ÙÙ‚Ø·.

## Ù…Ù‡Ù…ØªÙƒ
ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø³Ø¤Ø§Ù„ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠ ÙˆØªÙ‚Ø¯ÙŠÙ… Ø¥Ø¬Ø§Ø¨Ø© Ø¯Ù‚ÙŠÙ‚Ø© ÙˆÙ…Ø¨Ø±Ø±Ø© Ø¨Ø§Ù„Ù†ØµÙˆØµ.

## Ù‚ÙˆØ§Ø¹Ø¯ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø§Ù„Ø¥Ù„Ø²Ø§Ù…ÙŠØ©:
1. **Ù„Ø§ ØªØ®ØªÙ„Ù‚ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª**: Ø¥Ø°Ø§ Ù„Ù… ØªØ¬Ø¯ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© ÙÙŠ Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ù…Ù‚Ø¯Ù…Ø©ØŒ Ù‚Ù„ Ø°Ù„Ùƒ ØµØ±Ø§Ø­Ø©: "Ù„Ù… Ø£Ø¬Ø¯ Ù†ØµØ§Ù‹ ØµØ±ÙŠØ­Ø§Ù‹ ÙŠØ¬ÙŠØ¨ Ø¹Ù„Ù‰ Ù‡Ø°Ø§ Ø§Ù„Ø³Ø¤Ø§Ù„ ÙÙŠ Ø§Ù„Ù…ØµØ§Ø¯Ø± Ø§Ù„Ù…ØªØ§Ø­Ø©."
2. **Ø§Ù„Ø§Ù‚ØªØ¨Ø§Ø³ Ø§Ù„Ø¯Ù‚ÙŠÙ‚**: Ø¹Ù†Ø¯ Ø§Ù„Ø§Ø³ØªØ´Ù‡Ø§Ø¯ Ø¨Ù…Ø§Ø¯Ø© Ù‚Ø§Ù†ÙˆÙ†ÙŠØ©ØŒ Ø§Ù‚ØªØ¨Ø³ Ù†ØµÙ‡Ø§ Ø§Ù„Ø­Ø±ÙÙŠ Ø¨ÙŠÙ† Ø¹Ù„Ø§Ù…Ø§Øª ØªÙ†ØµÙŠØµ Â« Â».
3. **Ø§Ù„Ø¥Ø´Ø§Ø±Ø© Ù„Ù„Ù…ØµØ§Ø¯Ø±**: Ø§Ø³ØªØ®Ø¯Ù… Ø£Ø±Ù‚Ø§Ù… Ø§Ù„Ù…ØµØ§Ø¯Ø± [1], [2], [3] Ø¨Ø¹Ø¯ ÙƒÙ„ Ø§Ù‚ØªØ¨Ø§Ø³ Ø£Ùˆ Ù…Ø¹Ù„ÙˆÙ…Ø©.
4. **Ø§Ù„ØªÙ†Ø³ÙŠÙ‚ Ø§Ù„Ø¥Ù„Ø²Ø§Ù…ÙŠ**:
   - Ø§Ø¨Ø¯Ø£ Ø¨Ù€ **## Ù…Ù„Ø®Øµ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©** (3 Ø£Ø³Ø·Ø± ÙƒØ­Ø¯ Ø£Ù‚ØµÙ‰)
   - Ø«Ù… **## Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠ** Ù…Ø¹ Ø´Ø±Ø­ Ù…ÙØµÙ„ ÙˆØ£Ø±Ù‚Ø§Ù… Ø§Ù„Ù…ØµØ§Ø¯Ø±
   - Ø§Ø®ØªÙ… Ø¨Ù€ **## Ø§Ù„Ù…Ø±Ø§Ø¬Ø¹** (Ù‚Ø§Ø¦Ù…Ø© Ù…Ø±Ù‚Ù…Ø©: Ø§Ø³Ù… Ø§Ù„Ù‚Ø§Ù†ÙˆÙ† + Ø±Ù‚Ù… Ø§Ù„Ù…Ø§Ø¯Ø© Ø¥Ù† ÙˆØ¬Ø¯)
5. **Ø§Ù„Ù„ØºØ©**: Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø§Ù„ÙØµØ­Ù‰ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© Ø§Ù„Ø±Ø³Ù…ÙŠØ©. Ù„Ø§ ØªØ³ØªØ®Ø¯Ù… Ø§Ù„Ø¹Ø§Ù…ÙŠØ© Ø£Ø¨Ø¯Ø§Ù‹.
6. **Ø§Ù„Ù…ÙˆØ¶ÙˆØ¹ÙŠØ©**: Ù„Ø§ ØªÙØ¨Ø¯Ù Ø±Ø£ÙŠØ§Ù‹ Ø´Ø®ØµÙŠØ§Ù‹. Ø§Ù„ØªØ²Ù… Ø¨Ù…Ø§ Ø¬Ø§Ø¡ ÙÙŠ Ø§Ù„Ù†ØµÙˆØµ.

## Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© Ø§Ù„Ù…ØªØ§Ø­Ø©:
{context}

## Ø§Ù„Ø³Ø¤Ø§Ù„:
{query}

## Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©:"""

        try:
            response = generate_with_retry(self.model, prompt)
            answer = response.text.replace('"]', '"]\n') # Hack for ref formatting
        except Exception as e:
            print(f"Generation failed after retries: {e}")
            answer = "Ø¹Ø°Ø±Ø§Ù‹ØŒ Ø§Ù„Ù†Ø¸Ø§Ù… Ù…Ø´ØºÙˆÙ„ Ø¬Ø¯Ø§Ù‹ Ø­Ø§Ù„ÙŠØ§Ù‹ (Ø¶ØºØ· Ø¹Ù„Ù‰ Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„). Ù‡Ø°Ù‡ Ù‡ÙŠ Ø§Ù„Ù…ØµØ§Ø¯Ø± Ø§Ù„ØªÙŠ ÙˆØ¬Ø¯ØªÙ‡Ø§ØŒ Ù„ÙƒÙ† Ù„Ù… Ø£ØªÙ…ÙƒÙ† Ù…Ù† ØµÙŠØ§ØºØ© Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©. ÙŠØ±Ø¬Ù‰ Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© Ø¨Ø¹Ø¯ Ù‚Ù„ÙŠÙ„."
        
        return {
            "query": query, 
            "answer": answer, 
            "sources": [{
                "filename": m.get('filename'),
                "document_id": m.get('document_id'),
                "chunk_index": m.get('chunk_index', i+1),
                "content_preview": final_docs[i][:150] + "..." if len(final_docs[i]) > 150 else final_docs[i]
            } for i, m in enumerate(final_metas)]
        }

    def _extract_search_query(self, situation: str) -> str:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø°ÙƒÙŠ Ù„Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… LLM Ù„ØªØ­Ø³ÙŠÙ† Ø¯Ù‚Ø© Ø§Ù„Ø¨Ø­Ø«"""
        try:
            prompt = f"""Ø£Ù†Øª Ø®Ø¨ÙŠØ± Ù‚Ø§Ù†ÙˆÙ†ÙŠ Ø°ÙƒÙŠ. Ù…Ù‡Ù…ØªÙƒ Ù‡ÙŠ ØªØ­Ù„ÙŠÙ„ Ù…ÙˆÙ‚Ù Ù‚Ø§Ù†ÙˆÙ†ÙŠ ÙˆØ§Ø³ØªØ®Ø±Ø§Ø¬ Ø£ÙØ¶Ù„ ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ø¨Ø­Ø« Ù„Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø§Ù„Ù‚ÙˆØ§Ù†ÙŠÙ† ÙˆØ§Ù„Ù…Ø±Ø§Ø¬Ø¹ Ø§Ù„Ù…Ù†Ø§Ø³Ø¨Ø©.
            
Ø§Ù„Ù…ÙˆÙ‚Ù:
{situation[:2000]}

Ø§Ø³ØªØ®Ø±Ø¬ Ù…Ø§ ÙŠÙ„ÙŠ ÙÙŠ Ø³Ø·Ø± ÙˆØ§Ø­Ø¯ ÙÙ‚Ø·:
1. Ù†ÙˆØ¹ Ø§Ù„Ù‚Ø¶ÙŠØ© (Ù…Ø«Ù„Ø§Ù‹: Ù…ÙŠØ±Ø§Ø«ØŒ Ø¹Ù…Ù„ØŒ Ø¬Ù†Ø§Ø¦ÙŠØŒ Ø£Ø­ÙˆØ§Ù„ Ø´Ø®ØµÙŠØ©)
2. 5 Ø¥Ù„Ù‰ 10 ÙƒÙ„Ù…Ø§Øª Ù…ÙØªØ§Ø­ÙŠØ© Ø¯Ù‚ÙŠÙ‚Ø© Ù„Ù„Ø¨Ø­Ø« ÙÙŠ Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© (Ù…Ø«Ù„Ø§Ù‹: Ø§Ø³ØªØ®Ø¯Ù… "ØªØ³Ø±ÙŠØ­ ØªØ¹Ø³ÙÙŠ" Ø¨Ø¯Ù„Ø§Ù‹ Ù…Ù† "Ø·Ø±Ø¯"ØŒ "Ù‚Ø³Ù…Ø© ØªØ±ÙƒØ©" Ø¨Ø¯Ù„Ø§Ù‹ Ù…Ù† "ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ø¥Ø±Ø«")

ØªÙ†Ø³ÙŠÙ‚ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨:
[Ù†ÙˆØ¹ Ø§Ù„Ù‚Ø¶ÙŠØ©] ÙƒÙ„Ù…Ø§Øª Ù…ÙØªØ§Ø­ÙŠØ©

Ø£Ø¬Ø¨ ÙÙ‚Ø· Ø¨Ø§Ù„Ø³Ø·Ø± Ø§Ù„Ù…Ø·Ù„ÙˆØ¨ Ø¨Ø¯ÙˆÙ† Ø£ÙŠ Ù…Ù‚Ø¯Ù…Ø§Øª Ø£Ùˆ Ø´Ø±Ø­."""

            # Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„ Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ©
            # Ù†Ø³ØªØ®Ø¯Ù… Ù†ÙØ³ Ø¯Ø§Ù„Ø© Ø§Ù„ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ù…ÙˆØ«ÙˆÙ‚Ø© Ù„Ø¯ÙŠÙ†Ø§
            response = generate_with_retry(self.model, prompt)
            
            if response and hasattr(response, 'text'):
                extracted_text = response.text.strip()
                print(f"[Smart Extract] LLM Output: {extracted_text}")
                
                # Ø¯Ù…Ø¬ ÙˆØµÙ Ø§Ù„Ù…ÙˆÙ‚Ù (Ø£ÙˆÙ„ 50 ÙƒÙ„Ù…Ø© Ù„Ù„Ø³ÙŠØ§Ù‚) Ù…Ø¹ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…Ø³ØªØ®Ø±Ø¬Ø© Ø°ÙƒÙŠØ§Ù‹
                # Ù‡Ø°Ø§ ÙŠØ¶Ù…Ù† ÙˆØ¬ÙˆØ¯ Ø§Ù„Ø³ÙŠØ§Ù‚ Ø§Ù„Ø£ØµÙ„ÙŠ + Ø§Ù„Ù…ØµØ·Ù„Ø­Ø§Øª Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© Ø§Ù„Ø¯Ù‚ÙŠÙ‚Ø©
                words = situation.split()[:50]
                combined_query = " ".join(words) + " " + extracted_text
                return combined_query
            
            print("[Smart Extract] Empty response, falling back.")
            return " ".join(situation.split()[:80])

        except Exception as e:
            print(f"[Smart Extract] Error: {e}")
            # Fallback to simple truncation
            return " ".join(situation.split()[:80])

    def consult(self, situation: str):
        """ÙˆØ¶Ø¹ Ø§Ù„Ù…Ø³ØªØ´Ø§Ø± Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠ - Ø§Ø³ØªØ´Ø§Ø±Ø© Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© Ø§Ø­ØªØ±Ø§ÙÙŠØ©"""
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ø³ØªØ¹Ù„Ø§Ù… Ø¨Ø­Ø« Ù…Ø±ÙƒØ² Ù…Ù† Ø§Ù„Ù…ÙˆÙ‚Ù
        search_query = self._extract_search_query(situation)
        
        # Search for relevant laws AND jurisprudence using focused query
        docs, metas = self._retrieve(search_query, top_k=20)
        
        # Rerank using original situation for context relevance
        reranked = rerank_with_gemini(situation, docs, top_k=5)
        final_docs = [r[0] for r in reranked]
        final_metas = []
        doc_map = {d: m for d, m in zip(docs, metas)}
        for d in final_docs: final_metas.append(doc_map.get(d, {}))
        
        # Format context with source type indication (full text like Legal Search)
        context = ""
        for i, (doc, meta) in enumerate(zip(final_docs, final_metas), 1):
            source_name = meta.get('filename', f'Ù…ØµØ¯Ø± {i}').replace('.txt', '')
            source_type = "Ø§Ø¬ØªÙ‡Ø§Ø¯ Ù‚Ø¶Ø§Ø¦ÙŠ" if "Ù‚Ø±Ø§Ø±" in source_name or "Ø§Ø¬ØªÙ‡Ø§Ø¯" in source_name else "Ù†Øµ Ù‚Ø§Ù†ÙˆÙ†ÙŠ"
            context += f"\n\n### [{source_type} - Ù…ØµØ¯Ø± {i}: {source_name}]\n{doc}\n"
        
        # Professional Legal Consultant Prompt (v2.0)
        prompt = f"""Ø£Ù†Øª **Ù…Ø­Ø§Ù…Ù Ø£ÙˆÙ„ Ù…Ø¹ØªÙ…Ø¯ Ù„Ø¯Ù‰ Ø§Ù„Ù…Ø­ÙƒÙ…Ø© Ø§Ù„Ø¹Ù„ÙŠØ§ Ø§Ù„Ø¬Ø²Ø§Ø¦Ø±ÙŠØ©** Ø¨Ø®Ø¨Ø±Ø© 20 Ø³Ù†Ø© ÙÙŠ Ø§Ù„ØªØ±Ø§ÙØ¹ ÙˆØ§Ù„Ø§Ø³ØªØ´Ø§Ø±Ø§Øª.
ØªÙ‚Ø¯Ù… Ø§Ø³ØªØ´Ø§Ø±Ø§Øª Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© Ø¯Ù‚ÙŠÙ‚Ø© ÙˆÙ…Ù‡Ù†ÙŠØ© Ù„Ù„Ù…ÙˆÙƒÙ„ÙŠÙ†ØŒ Ù…Ø³ØªÙ†Ø¯Ø§Ù‹ Ø¥Ù„Ù‰ Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© ÙˆØ§Ø¬ØªÙ‡Ø§Ø¯Ø§Øª Ø§Ù„Ù…Ø­ÙƒÙ…Ø© Ø§Ù„Ø¹Ù„ÙŠØ§.

## Ø§Ù„Ù…ÙˆÙ‚Ù Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠ Ø§Ù„Ù…Ø¹Ø±ÙˆØ¶:
{situation}

## Ø§Ù„Ù…ØµØ§Ø¯Ø± Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© Ø§Ù„Ù…ØªØ§Ø­Ø©:
(ØªØ´Ù…Ù„ Ù†ØµÙˆØµ Ø§Ù„Ù‚ÙˆØ§Ù†ÙŠÙ†ØŒ Ø§Ø¬ØªÙ‡Ø§Ø¯Ø§Øª Ø§Ù„Ù…Ø­ÙƒÙ…Ø© Ø§Ù„Ø¹Ù„ÙŠØ§ØŒ ÙˆÙ‚Ø±Ø§Ø±Ø§Øª Ù…Ø¬Ù„Ø³ Ø§Ù„Ø¯ÙˆÙ„Ø©)
{context}

---

## Ù‡ÙŠÙƒÙ„Ø© Ø§Ù„Ø§Ø³ØªØ´Ø§Ø±Ø© (Ø§Ù„ØªØ²Ù… Ø¨Ù‡Ø°Ø§ Ø§Ù„ØªØ±ØªÙŠØ¨):

### 1. Ø§Ù„ØªÙƒÙŠÙŠÙ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠ âš–ï¸
- Ù…Ø§ Ù‡Ùˆ Ù†ÙˆØ¹ Ø§Ù„Ù‚Ø¶ÙŠØ©ØŸ (Ù…Ø¯Ù†ÙŠ / Ø¬Ø²Ø§Ø¦ÙŠ / Ø¥Ø¯Ø§Ø±ÙŠ / Ø£Ø³Ø±Ø© / Ø¹Ù…Ù„ / ØªØ¬Ø§Ø±ÙŠ)
- Ù…Ø§ Ù‡ÙŠ Ø§Ù„ÙˆÙ‚Ø§Ø¦Ø¹ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© Ø§Ù„Ù…Ø¤Ø«Ø±Ø©ØŸ
- Ù…Ù† Ù‡Ù… Ø§Ù„Ø£Ø·Ø±Ø§Ù ÙˆÙ…Ø§ ØµÙØ§ØªÙ‡Ù… Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ©ØŸ

### 2. Ø§Ù„Ø£Ø³Ø§Ø³ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠ ğŸ“š
- **Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ©**: Ø§Ø°ÙƒØ± Ø§Ù„Ù…ÙˆØ§Ø¯ Ø§Ù„Ù…Ù†Ø·Ø¨Ù‚Ø© Ù…Ø¹ Ù†ØµÙ‡Ø§ Ø¨ÙŠÙ† Â« Â» [Ø±Ù‚Ù… Ø§Ù„Ù…ØµØ¯Ø±]
- **Ø§Ù„Ø§Ø¬ØªÙ‡Ø§Ø¯ Ø§Ù„Ù‚Ø¶Ø§Ø¦ÙŠ**: Ø¥Ù† ÙˆØ¬Ø¯ Ù‚Ø±Ø§Ø± Ù…Ù† Ø§Ù„Ù…Ø­ÙƒÙ…Ø© Ø§Ù„Ø¹Ù„ÙŠØ§ ÙŠØ¯Ø¹Ù… Ø§Ù„Ù…ÙˆÙ‚ÙØŒ Ø§Ø°ÙƒØ±Ù‡

### 3. Ø§Ù„ØªÙˆØ¬ÙŠÙ‡ Ø§Ù„Ø¹Ù…Ù„ÙŠ ğŸ¯
- Ù…Ø§ Ø§Ù„Ø¥Ø¬Ø±Ø§Ø¡ Ø§Ù„ÙˆØ§Ø¬Ø¨ØŸ (Ø´ÙƒÙˆÙ‰ / Ø¯Ø¹ÙˆÙ‰ / ØµÙ„Ø­ / ØªØ¸Ù„Ù… / Ø§Ø³ØªØ¦Ù†Ø§Ù)
- Ø£Ù…Ø§Ù… Ø£ÙŠ Ø¬Ù‡Ø©ØŸ (Ù…Ø­ÙƒÙ…Ø© Ø§Ø¨ØªØ¯Ø§Ø¦ÙŠØ© / Ù…Ø¬Ù„Ø³ Ù‚Ø¶Ø§Ø¦ÙŠ / Ù…Ø­ÙƒÙ…Ø© Ø¹Ù„ÙŠØ§ / Ø¥Ø¯Ø§Ø±Ø©)
- Ù…Ø§ Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚ ÙˆØ§Ù„Ø£Ø¯Ù„Ø© Ø§Ù„Ù„Ø§Ø²Ù…Ø©ØŸ

### 4. Ø§Ù„ØªØ­Ø°ÙŠØ±Ø§Øª Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© âš ï¸
- Ø¢Ø¬Ø§Ù„ Ø§Ù„ØªÙ‚Ø§Ø¯Ù… Ø£Ùˆ Ø§Ù„Ø³Ù‚ÙˆØ· (Ø¥Ù† ÙˆØ¬Ø¯Øª)
- Ø§Ù„Ù…Ø®Ø§Ø·Ø± Ø§Ù„Ù…Ø­ØªÙ…Ù„Ø© Ø¥Ù† Ù„Ù… ÙŠÙØªØ®Ø° Ø¥Ø¬Ø±Ø§Ø¡ Ø³Ø±ÙŠØ¹
- Ù†Ù‚Ø§Ø· Ø¶Ø¹Ù Ø§Ù„Ù…ÙˆÙ‚Ù Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠ (Ø¥Ù† ÙˆØ¬Ø¯Øª)

### 5. Ø§Ù„Ø®Ù„Ø§ØµØ© ÙˆØ§Ù„ØªÙˆØµÙŠØ© ğŸ“Œ
- Ù…Ù„Ø®Øµ Ø§Ù„Ù…ÙˆÙ‚Ù ÙÙŠ 3 Ø£Ø³Ø·Ø± ÙƒØ­Ø¯ Ø£Ù‚ØµÙ‰
- Ø§Ù„ØªÙˆØµÙŠØ© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ© Ø§Ù„ÙˆØ§Ø¶Ø­Ø©

---
âš ï¸ **ØªÙ†ÙˆÙŠÙ‡**: Ù‡Ø°Ù‡ Ø§Ø³ØªØ´Ø§Ø±Ø© Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© Ø£ÙˆÙ„ÙŠØ© Ù…Ø¨Ù†ÙŠØ© Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù…Ù‚Ø¯Ù…Ø©. ÙŠÙÙ†ØµØ­ Ø¨Ù…Ø±Ø§Ø¬Ø¹Ø© Ù…Ø­Ø§Ù…Ù Ù…ØªØ®ØµØµ Ù„Ø¯Ø±Ø§Ø³Ø© Ù…Ù„Ù Ø§Ù„Ù‚Ø¶ÙŠØ© ÙƒØ§Ù…Ù„Ø§Ù‹."""

        try:
            response = generate_with_retry(self.model, prompt)
            consultation_text = response.text
        except Exception as e:
            print(f"Consultation generation failed: {e}")
            consultation_text = "Ø¹Ø°Ø±Ø§Ù‹ØŒ Ù„Ù… Ø£ØªÙ…ÙƒÙ† Ù…Ù† ØµÙŠØ§ØºØ© Ø§Ù„Ø§Ø³ØªØ´Ø§Ø±Ø© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ© Ø¨Ø³Ø¨Ø¨ Ø¶ØºØ· Ø§Ù„Ù†Ø¸Ø§Ù…. ÙŠØ±Ø¬Ù‰ Ù…Ø±Ø§Ø¬Ø¹Ø© Ø§Ù„Ù…ØµØ§Ø¯Ø± Ø£Ø¯Ù†Ø§Ù‡."

        return {
            "consultation": consultation_text,
            "sources": [{"filename": m.get('filename')} for m in final_metas]
        }

    def draft_pleading(self, case_data: dict, pleading_type="Ø¯ÙØ§Ø¹", style="formel", top_k=30):
        """
        ÙˆØ¶Ø¹ Ø§Ù„Ù…Ø­Ø§Ù…ÙŠ: ØªÙˆÙ„ÙŠØ¯ Ù…Ø°ÙƒØ±Ø§Øª Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© Ø§Ø­ØªØ±Ø§ÙÙŠØ© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù‡ÙŠÙƒÙ„ÙŠØ© Ø§Ù„Ù…Ø±Ø§ÙØ¹Ø§Øª Ø§Ù„Ø°Ù‡Ø¨ÙŠØ©
        Advocate Mode: Generate professional legal pleadings
        """
        facts = case_data.get('facts', '')
        charges = " ".join(case_data.get('charges', []))
        defendant_name = case_data.get('defendant_name', 'Ø§Ù„Ù…ØªÙ‡Ù…')
        court = case_data.get('court', 'Ø§Ù„Ù…Ø­ÙƒÙ…Ø© Ø§Ù„Ù…Ø®ØªØµØ©')
        case_number = case_data.get('case_number', '')
        
        # 1. Smart Extraction from Case Data
        # Combine facts and charges for context extraction
        case_context = f"Ø§Ù„ØªÙ‡Ù…Ø©: {charges}. Ø§Ù„ÙˆÙ‚Ø§Ø¦Ø¹: {facts}"
        search_query = self._extract_search_query(case_context)
        print(f"[Pleading] Smart Query: {search_query}")

        # 2. Retrieval
        docs, metas = self._retrieve(search_query, top_k=top_k)
        
        # 3. Reranking using Gemini
        # Select best 5 sources relevant to the case facts
        reranked = rerank_with_gemini(case_context, docs, top_k=5)
        final_docs = [r[0] for r in reranked]
        final_metas = []
        
        # Match metadata back to reranked docs
        doc_map = {d: m for d, m in zip(docs, metas)}
        for d in final_docs: final_metas.append(doc_map.get(d, {}))
        
        # 4. Build Legal Context (Full Text - No Truncation)
        context = ""
        for i, (doc, meta) in enumerate(zip(final_docs, final_metas), 1):
            source_name = meta.get('filename', f'Ù…ØµØ¯Ø± {i}').replace('.txt', '')
            source_type = "Ø§Ø¬ØªÙ‡Ø§Ø¯ Ù‚Ø¶Ø§Ø¦ÙŠ" if "Ù‚Ø±Ø§Ø±" in source_name or "Ø§Ø¬ØªÙ‡Ø§Ø¯" in source_name else "Ù†Øµ Ù‚Ø§Ù†ÙˆÙ†ÙŠ"
            context += f"\n\n### [{source_type} - Ù…ØµØ¯Ø± {i}: {source_name}]\n{doc}\n"
    
        # 5. Professional "Golden Pleading" Prompt
        prompt = f"""Ø£Ù†Øª Ù…Ø­Ø§Ù…Ù Ø¬Ø²Ø§Ø¦Ø±ÙŠ Ø®Ø¨ÙŠØ± "Ù†Ø§Ø¨Øº" (Top-Tier Lawyer) Ø£Ù…Ø§Ù… Ø§Ù„Ù…Ø­ÙƒÙ…Ø© Ø§Ù„Ø¹Ù„ÙŠØ§ ÙˆÙ…Ø¬Ù„Ø³ Ø§Ù„Ø¯ÙˆÙ„Ø©.
Ù…Ù‡Ù…ØªÙƒ: ØµÙŠØ§ØºØ© **{pleading_type}** Ø§Ø­ØªØ±Ø§ÙÙŠØ© Ø¬Ø¯Ø§Ù‹ ØªØ­Ø§ÙƒÙŠ "Ø§Ù„Ù…Ø±Ø§ÙØ¹Ø§Øª Ø§Ù„Ø°Ù‡Ø¨ÙŠØ©" Ù…Ù† Ø­ÙŠØ« Ø§Ù„Ø¨Ù„Ø§ØºØ©ØŒ Ø§Ù„Ø­Ø¬Ø© Ø§Ù„Ø¯Ø§Ù…ØºØ©ØŒ ÙˆØ§Ù„Ù‡ÙŠÙƒÙ„Ø© Ø§Ù„ØµØ§Ø±Ù…Ø©.

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ“‹ Ù…Ù„Ù Ø§Ù„Ù‚Ø¶ÙŠØ©
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
â€¢ Ø§Ù„Ø¬Ù‡Ø© Ø§Ù„Ù‚Ø¶Ø§Ø¦ÙŠØ©: {court}
â€¢ Ø±Ù‚Ù… Ø§Ù„Ù…Ù„Ù: {case_number}
â€¢ Ø§Ù„Ø£Ø·Ø±Ø§Ù: Ø§Ù„Ù…ØªÙ‡Ù… {defendant_name} Ø¶Ø¯ Ø§Ù„Ù†ÙŠØ§Ø¨Ø© Ø§Ù„Ø¹Ø§Ù…Ø©/Ø§Ù„Ø¶Ø­ÙŠØ©
â€¢ Ø§Ù„ØªÙ‡Ù…Ø©: {charges if charges else 'ØºÙŠØ± Ù…Ø­Ø¯Ø¯Ø©'}

ğŸ“ ÙˆÙ‚Ø§Ø¦Ø¹ Ø§Ù„Ù‚Ø¶ÙŠØ© (ÙƒÙ…Ø§ ÙˆØ±Ø¯Øª ÙÙŠ Ø§Ù„Ù…Ù„Ù):
{facts}

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ“š Ø§Ù„Ø°Ø®ÙŠØ±Ø© Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© (Ø§Ù„Ø³Ù†Ø¯)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
Ø§Ø³ØªØ®Ø¯Ù… Ù‡Ø°Ù‡ Ø§Ù„Ù†ØµÙˆØµ Ø¨Ø°ÙƒØ§Ø¡ Ù„ØªØ¯Ø¹ÙŠÙ… Ø¯ÙÙˆØ¹Ùƒ (ÙŠØ¬Ø¨ Ø°ÙƒØ± Ø±Ù‚Ù… Ø§Ù„Ù…Ø§Ø¯Ø© ÙˆØ§Ø³Ù… Ø§Ù„Ù‚Ø§Ù†ÙˆÙ† Ø¨Ø¯Ù‚Ø©). Ù„Ø§ ØªØ°ÙƒØ± Ù†ØµÙˆØµØ§Ù‹ Ù„Ù… ØªØ±Ø¯ Ù‡Ù†Ø§ Ø¥Ù„Ø§ Ø¥Ø°Ø§ ÙƒÙ†Øª Ù…ØªØ£ÙƒØ¯Ø§Ù‹ Ù…Ù†Ù‡Ø§ 100%:
{context}

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
âš–ï¸ Ø§Ù„Ù‡ÙŠÙƒÙ„Ø© Ø§Ù„Ø°Ù‡Ø¨ÙŠØ© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø© (Ø¥Ù„Ø²Ø§Ù…ÙŠØ©)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ÙŠØ¬Ø¨ Ø£Ù† ØªØªØ¨Ø¹ Ù…Ø°ÙƒØ±ØªÙƒ Ø§Ù„Ù‡ÙŠÙƒÙ„ Ø§Ù„ØªØ§Ù„ÙŠ Ø¨Ø¯Ù‚Ø©:

1. **Ø§Ù„Ø¯ÙŠØ¨Ø§Ø¬Ø© Ø§Ù„Ø§Ø­ØªØ±Ø§ÙÙŠØ©**: (Ø¥Ù„Ù‰ Ø§Ù„Ø³ÙŠØ¯ Ø±Ø¦ÙŠØ³ Ø§Ù„Ù…Ø­ÙƒÙ…Ø©... Ù„ÙØ§Ø¦Ø¯Ø© Ø§Ù„Ù…ØªÙ‡Ù…... Ø¶Ø¯...)
2. **Ø£ÙˆÙ„Ø§Ù‹: Ù…ÙˆØ¬Ø² Ø§Ù„ÙˆÙ‚Ø§Ø¦Ø¹**: (ØµÙŠØ§ØºØ© Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© Ù…Ø­Ø§ÙŠØ¯Ø© ÙˆÙ…Ø±ÙƒØ²Ø© Ù„Ù„ÙˆÙ‚Ø§Ø¦Ø¹).
3. **Ø«Ø§Ù†ÙŠØ§Ù‹: Ø§Ù„Ø¥Ø¬Ø±Ø§Ø¡Ø§Øª**: (Ø°ÙƒØ± Ø§Ù„Ø¥Ø¬Ø±Ø§Ø¡Ø§Øª Ø§Ù„Ù…ØªØ®Ø°Ø© Ø¨Ø§Ø®ØªØµØ§Ø±).
4. **Ø«Ø§Ù„Ø«Ø§Ù‹: Ø§Ù„Ù…Ù†Ø§Ù‚Ø´Ø© Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© (Ù‚Ù„Ø¨ Ø§Ù„Ù…Ø±Ø§ÙØ¹Ø©)**:
   Ø£- **ÙÙŠ Ø§Ù„Ø´ÙƒÙ„ (Ø§Ù„Ø¯ÙÙˆØ¹ Ø§Ù„Ø´ÙƒÙ„ÙŠØ©)**: (ØªØ­Ù‚Ù‚ Ù…Ù†: Ø¨Ø·Ù„Ø§Ù† Ø§Ù„Ø¥Ø¬Ø±Ø§Ø¡Ø§ØªØŒ Ø§Ù„ØªÙ‚Ø§Ø¯Ù…ØŒ Ø§Ù„Ø§Ø®ØªØµØ§Øµ - Ø¥Ù† ÙˆØ¬Ø¯Øª Ø«ØºØ±Ø§Øª).
   Ø¨- **ÙÙŠ Ø§Ù„Ù…ÙˆØ¶ÙˆØ¹ (Ø§Ù„Ø¯ÙÙˆØ¹ Ø§Ù„Ù…ÙˆØ¶ÙˆØ¹ÙŠØ©)**:
      - Ù…Ù†Ø§Ù‚Ø´Ø© Ø£Ø±ÙƒØ§Ù† Ø§Ù„ØªÙ‡Ù…Ø© (Ø§Ù„Ù…Ø§Ø¯ÙŠ ÙˆØ§Ù„Ù…Ø¹Ù†ÙˆÙŠ) ÙˆØªÙÙ†ÙŠØ¯Ù‡Ø§.
      - ØªØ­Ù„ÙŠÙ„ Ø£Ø¯Ù„Ø© Ø§Ù„Ø¥Ø«Ø¨Ø§Øª ÙˆÙƒØ´Ù Ø§Ù„ØªÙ†Ø§Ù‚Ø¶Ø§Øª.
      - Ø§Ø³ØªØ«Ù…Ø§Ø± Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© {context} Ù„ØµØ§Ù„Ø­ Ø§Ù„Ù…ÙˆÙƒÙ„.
      - Ø§Ù„Ø§Ø³ØªØ´Ù‡Ø§Ø¯ Ø¨Ø§Ù„Ø§Ø¬ØªÙ‡Ø§Ø¯ Ø§Ù„Ù‚Ø¶Ø§Ø¦ÙŠ (Ø¥Ù† ÙˆØ¬Ø¯ ÙÙŠ Ø§Ù„Ù…ØµØ§Ø¯Ø±).
5. **Ø±Ø§Ø¨Ø¹Ø§Ù‹: Ø§Ù„Ø·Ù„Ø¨Ø§Øª Ø§Ù„Ø®ØªØ§Ù…ÙŠØ©**: (Ø£ØµÙ„ÙŠØ§Ù‹: Ø§Ù„Ø¨Ø±Ø§Ø¡Ø©/Ø§Ù„Ø¥Ù„ØºØ§Ø¡ØŒ Ø§Ø­ØªÙŠØ§Ø·ÙŠØ§Ù‹: Ø§Ù„ØªØ®ÙÙŠÙ/Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„ØªÙƒÙŠÙŠÙ).

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ’¡ ØªÙˆØ¬ÙŠÙ‡Ø§Øª Ø§Ù„Ø£Ø³Ù„ÙˆØ¨ ("Ø§Ù„Ù„Ù…Ø³Ø© Ø§Ù„Ø°Ù‡Ø¨ÙŠØ©")
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
- Ø§Ø³ØªØ®Ø¯Ù… Ù„ØºØ© Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© Ø¬Ø²ÙŠÙ„Ø© ÙˆØ±ØµÙŠÙ†Ø© (Ù…Ø«Ø§Ù„: "Ø­ÙŠØ« Ø£Ù† Ø§Ù„Ø«Ø§Ø¨Øª..."ØŒ "ÙˆÙ„Ù…Ø§ ÙƒØ§Ù† Ù…Ù† Ø§Ù„Ù…Ù‚Ø±Ø±...").
- ÙƒÙ† Ù‡Ø¬ÙˆÙ…ÙŠØ§Ù‹ ÙÙŠ Ø§Ù„Ø­Ù‚ØŒ Ù…ÙÙ†Ø¯Ø§Ù‹ Ù„Ø£Ø¯Ù„Ø© Ø§Ù„Ø®ØµÙ… Ø¨Ø§Ù„Ø­Ø¬Ø© ÙˆØ§Ù„Ø¨Ø±Ù‡Ø§Ù†.
- Ø§Ø±Ø¨Ø· Ø§Ù„Ù†Øµ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠ Ø¨Ø§Ù„ÙˆØ§Ù‚Ø¹Ø© Ù…Ø¨Ø§Ø´Ø±Ø© ("Ø­ÙŠØ« Ø£Ù† Ø§Ù„Ù…Ø§Ø¯Ø© X ØªØ´ØªØ±Ø·... ÙˆØ­ÙŠØ« Ø£Ù† Ù…ÙˆÙƒÙ„ÙŠ Ù„Ù… ÙŠÙ‚Ù… Ø¨Ù€...").
- Ø§Ù„ØªØ²Ù… Ø¨Ø£Ø³Ù„ÙˆØ¨ {style}.

Ø§Ø¨Ø¯Ø£ ØµÙŠØ§ØºØ© Ø§Ù„Ù…Ø±Ø§ÙØ¹Ø© ÙÙˆØ±Ø§Ù‹:"""

        try:
            response = generate_with_retry(self.model, prompt)
            pleading_text = response.text
        except Exception as e:
            print(f"Pleading generation failed: {e}")
            pleading_text = f"""# Ù…Ø°ÙƒØ±Ø© {pleading_type}

âš ï¸ Ø¹Ø°Ø±Ø§Ù‹ØŒ Ù„Ù… Ø£ØªÙ…ÙƒÙ† Ù…Ù† Ø¥ØªÙ…Ø§Ù… ØµÙŠØ§ØºØ© Ø§Ù„Ù…Ø°ÙƒØ±Ø© Ø¨Ø³Ø¨Ø¨ Ø®Ø·Ø£ ØªÙ‚Ù†ÙŠ.

## Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù…ØªØ§Ø­Ø©:
- **Ø§Ù„Ù…ØªÙ‡Ù…**: {defendant_name}
- **Ø§Ù„ØªÙ‡Ù…Ø©**: {charges}
- **Ø§Ù„ÙˆÙ‚Ø§Ø¦Ø¹**: {facts[:200]}...

## Ø§Ù„Ù…ØµØ§Ø¯Ø± Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© Ø§Ù„Ù…Ø³ØªØ®Ø±Ø¬Ø©:
{context}

ÙŠØ±Ø¬Ù‰ Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø©."""

        return {
            "pleading": pleading_text,
            "metadata": {"total_sources": len(docs), "pleading_type": pleading_type},
            "sources": [{"filename": m.get('filename')} for m in final_metas[:10]]
        }


    def search_jurisprudence(self, legal_issue: str, chamber=None, top_k=20):
        # Jurisprudence Mode - Filter by both Supreme Court and Conseil d'Ã‰tat
        # Note: RPC match_documents does not support list filters, so we fetch all and filter in Python
        # filters = {"category": ["jurisprudence", "jurisprudence_conseil_etat"]}
        
        # If chamber is specified, append it to query
        search_query = legal_issue
        if chamber:
             search_query += f" ({chamber})"
             
        # Fetch broad (50) then filter
        raw_docs, raw_metas = self._retrieve(search_query, filters=None, top_k=50)
        
        docs = []
        metas = []
        target_categories = ["jurisprudence", "jurisprudence_conseil_etat"]
        
        for d, m in zip(raw_docs, raw_metas):
            if m.get("category") in target_categories:
                docs.append(d)
                metas.append(m)
        
        # Slice to requested top_k
        docs = docs[:top_k]
        metas = metas[:top_k]
        
        # Limit context to avoid token limit (Groq max ~12K tokens)
        truncated_docs = [d[:1200] for d in docs[:5]]  # 5 docs, 1200 chars each
        context = "\n".join([f"ArrÃªt {i+1}: {d}" for i, d in enumerate(truncated_docs)])
        
        prompt = f"""Ø¨ØµÙØªÙƒ Ø¨Ø§Ø­Ø«Ø§Ù‹ ÙÙŠ Ø§Ù„Ø§Ø¬ØªÙ‡Ø§Ø¯ Ø§Ù„Ù‚Ø¶Ø§Ø¦ÙŠ (Ø§Ù„Ù…Ø­ÙƒÙ…Ø© Ø§Ù„Ø¹Ù„ÙŠØ§ ÙˆÙ…Ø¬Ù„Ø³ Ø§Ù„Ø¯ÙˆÙ„Ø©).
Ø§Ù„Ù…Ø³Ø£Ù„Ø©: {legal_issue}
Ø§Ù„Ù‚Ø±Ø§Ø±Ø§Øª Ø§Ù„Ù…Ø³ØªØ®Ø±Ø¬Ø©:
{context}

Ø§Ù„Ù…Ø·Ù„ÙˆØ¨:
1. Ø§Ø³ØªØ®Ø±Ø¬ Ø§Ù„Ù…Ø¨Ø§Ø¯Ø¦ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© Ø¨Ø¯Ù‚Ø©.
2. Ù„ÙƒÙ„ Ù…Ø¨Ø¯Ø£ØŒ **ÙŠØ¬Ø¨** Ø¥Ø¯Ø±Ø§Ø¬ "Ù†Øµ Ø§Ù„Ù…Ø¨Ø¯Ø£" ÙƒÙ…Ø§ ÙˆØ±Ø¯ ÙÙŠ Ø§Ù„Ù‚Ø±Ø§Ø± Ø¨ÙŠÙ† Ø¹Ù„Ø§Ù…ØªÙŠ Ø§Ù‚ØªØ¨Ø§Ø³.
3. Ø§Ø°ÙƒØ± Ø±Ù‚Ù… Ø§Ù„Ù‚Ø±Ø§Ø± ÙˆØªØ§Ø±ÙŠØ®Ù‡ ÙˆØ§Ù„Ø¬Ù‡Ø© Ø§Ù„Ù…ØµØ¯Ø±Ø© (Ø§Ù„Ù…Ø­ÙƒÙ…Ø© Ø§Ù„Ø¹Ù„ÙŠØ§ Ø£Ùˆ Ù…Ø¬Ù„Ø³ Ø§Ù„Ø¯ÙˆÙ„Ø©) Ø¥Ù† ÙˆØ¬Ø¯ ÙÙŠ Ø§Ù„Ù†Øµ.
4. ÙˆØ¶Ø­ Ù‡Ù„ Ø§Ù„Ø§Ø¬ØªÙ‡Ø§Ø¯ Ù…Ø³ØªÙ‚Ø± Ø£Ù… Ù‡Ù†Ø§Ùƒ ØªÙ†Ø§Ù‚Ø¶.

Ø§Ù„ØªÙ†Ø³ÙŠÙ‚ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨:
- **Ø§Ù„Ù…Ø¨Ø¯Ø£:** [Ø´Ø±Ø­ Ø§Ù„Ù…Ø¨Ø¯Ø£]
- **Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ù‚ØªØ¨Ø³:** "...[Ø§Ù„Ù†Øµ]..."
- **Ø§Ù„Ù…Ø±Ø¬Ø¹:** Ù‚Ø±Ø§Ø± Ø±Ù‚Ù… [X] Ø¨ØªØ§Ø±ÙŠØ® [Y] - [Ø§Ù„Ø¬Ù‡Ø©] (Ø¥Ù† ÙˆØ¬Ø¯)"""

        response = generate_with_retry(self.model, prompt)
        
        # Include text snippets in sources for UI
        enriched_sources = []
        for doc, meta in zip(docs[:5], metas[:5]):
             enriched_sources.append({
                 "filename": meta.get('filename'),
                 "relevance_score": 0.9,
                 "snippet": doc[:200] + "..." # Snippet for UI
             })

        return {
            "analysis": response.text,
            "metadata": {"total_sources": len(docs)},
            "sources": enriched_sources
        }

rag_service = RAGService()

def rag_pipeline(query, filters=None, skip_generation=False):
    return rag_service.answer_query(query, filters, skip_generation)

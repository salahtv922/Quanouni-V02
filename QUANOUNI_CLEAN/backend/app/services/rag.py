import requests
import json
import re
import time
from dataclasses import dataclass
from typing import Optional
from app.core.config import settings
from app.services.embedding import get_embedding
from app.services.vector_store import query_chroma
# SDK removed to reduce bundle size for serverless (Vercel 250MB limit)
# import google.generativeai as genai



@dataclass
class GenerationResponse:
    text: str


def generate_with_retry(model, prompt, retries=5, delay=4):
    # Check if Groq is enabled (Preferred for Generation)
    if hasattr(settings, 'GROQ_API_KEY') and settings.GROQ_API_KEY:
        try:
            # Groq API Call
            headers = {
                "Authorization": f"Bearer {settings.GROQ_API_KEY}",
                "Content-Type": "application/json"
            }
            # Fallback to stable model if the configured one fails
            model_id = getattr(settings, 'GROQ_MODEL', None) or "llama-3.1-70b-versatile"
            print(f"[Groq] Using model: {model_id}")
            data = {
                "messages": [{"role": "user", "content": prompt}],
                "model": model_id,
                "temperature": 0.3,
                "max_tokens": 4096
            }
            
            # Simple retry logic for Groq too
            for attempt in range(3):
                try:
                    resp = requests.post("https://api.groq.com/openai/v1/chat/completions", headers=headers, json=data, timeout=120)
                    if resp.status_code == 200:
                        content = resp.json()['choices'][0]['message']['content']
                        return GenerationResponse(text=content)
                    elif resp.status_code == 429:
                        print(f"Groq Rate Limit, waiting {delay}s...")
                        time.sleep(delay)
                        continue
                    else:
                        print(f"Groq Error {resp.status_code}: {resp.text[:500]}")
                except requests.exceptions.Timeout:
                    print(f"Groq Timeout (attempt {attempt+1}/3)")
                    continue
                except Exception as e:
                    print(f"Groq Exception: {type(e).__name__}: {e}")
            
            print("Groq failed, falling back to Gemini...")
        except Exception as e:
             print(f"Groq Setup Error: {e}, falling back...")

    # Fallback to Gemini REST API
    url = f"https://generativelanguage.googleapis.com/v1beta/models/{settings.GEMINI_CHAT_MODEL}:generateContent?key={settings.GEMINI_API_KEY}"
    
    headers = {"Content-Type": "application/json"}
    payload = {
        "contents": [{"parts": [{"text": prompt}]}],
        "generationConfig": {
            "temperature": 0.7,
            "maxOutputTokens": 8192
        }
    }

    for attempt in range(retries):
        try:
            resp = requests.post(url, headers=headers, json=payload, timeout=60)
            if resp.status_code == 200:
                result = resp.json()
                # Extract text from Gemini response structure
                try:
                    text = result['candidates'][0]['content']['parts'][0]['text']
                    return GenerationResponse(text=text)
                except (KeyError, IndexError):
                     print(f"Gemini Bad Response format: {result}")
                     raise ValueError("Gemini response parsing failed")
            elif resp.status_code == 429:
                 print(f"Gemini Rate Limit, waiting {delay}s...")
                 time.sleep(delay)
                 delay *= 2
            else:
                 print(f"Gemini Error {resp.status_code}: {resp.text}")
                 if attempt == retries - 1:
                     raise Exception(f"Gemini API Error: {resp.text}")
                 
        except Exception as e:
            if attempt < retries - 1:
                print(f"Gemini Exception: {e}, retrying...")
                time.sleep(delay)
            else:
                raise e

def detect_language(text: str) -> str:
    """Detect the language of the query"""
    arabic_chars = sum(1 for c in text if '\u0600' <= c <= '\u06FF')
    total_chars = len([c for c in text if c.isalpha()])
    
    if total_chars == 0:
        return "ar"
    
    arabic_ratio = arabic_chars / total_chars
    
    if arabic_ratio > 0.3:
        return "ar"
    
    french_words = ['le', 'la', 'les', 'de', 'et', 'dans', 'pour', 'sont', 'peut', 'comment', 'quels', 'quel']
    text_lower = text.lower()
    if any(word in text_lower.split() for word in french_words):
        return "fr"
    
    return "en"

def rerank_with_gemini(query: str, chunks: list[str], top_k: int = 3) -> list[tuple[str, float]]:
    # Removed configure_gemini() call
    # model = genai.GenerativeModel(...) -> Just pass None as we use REST inside generate_with_retry
    model = None 

    
    chunks_text = ""
    for i, chunk in enumerate(chunks[:10], 1):
        chunks_text += f"\n\n### Chunk {i}:\n{chunk[:500]}...\n"
    
    prompt = f"""Ø£Ù†Øª Ø®Ø¨ÙŠØ± Ù‚Ø§Ù†ÙˆÙ†ÙŠ Ø¬Ø²Ø§Ø¦Ø±ÙŠ. Ù…Ù‡Ù…ØªÙƒ ØªØ±ØªÙŠØ¨ Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© Ø­Ø³Ø¨ ØµÙ„ØªÙ‡Ø§ Ø¨Ø§Ù„Ø³Ø¤Ø§Ù„.

Ø§Ù„Ø³Ø¤Ø§Ù„: {query}

Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ù…ØªØ§Ø­Ø©:
{chunks_text}

Ù…Ø¹Ø§ÙŠÙŠØ± Ø§Ù„ØªÙ‚ÙŠÙŠÙ… (Ù…Ù‡Ù… Ø¬Ø¯Ø§Ù‹):
- 10: Ø§Ù„Ù…Ø§Ø¯Ø© Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© Ø§Ù„ØªÙŠ ØªÙØ¹Ø±ÙÙ‘Ù Ø§Ù„Ø¬Ø±ÙŠÙ…Ø© Ø£Ùˆ ØªØ­Ø¯Ø¯ Ø§Ù„Ø¹Ù‚ÙˆØ¨Ø© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø© Ù…Ø¨Ø§Ø´Ø±Ø©
- 9-10: *Ø§Ù„Ø§Ø¬ØªÙ‡Ø§Ø¯ Ø§Ù„Ù‚Ø¶Ø§Ø¦ÙŠ* (Ù‚Ø±Ø§Ø± Ø§Ù„Ù…Ø­ÙƒÙ…Ø© Ø§Ù„Ø¹Ù„ÙŠØ§/Ù…Ø¬Ù„Ø³ Ø§Ù„Ø¯ÙˆÙ„Ø©) Ø§Ù„Ø°ÙŠ ÙŠÙØµÙ„ ÙÙŠ Ù†ÙØ³ Ø§Ù„Ù…Ø³Ø£Ù„Ø© Ø¨Ø¯Ù‚Ø©
- 8-9: Ù…Ø§Ø¯Ø© Ù…Ù† Ù†ÙØ³ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ† ØªØªØ­Ø¯Ø« Ø¹Ù† Ù†ÙØ³ Ø§Ù„Ù…ÙˆØ¶ÙˆØ¹ (Ù…Ø«Ù„Ø§Ù‹: Ø³Ø±Ù‚Ø©ØŒ Ø·Ù„Ø§Ù‚ØŒ Ø¹Ù‚Ø¯)
- 5-7: Ù…Ø§Ø¯Ø© Ø£Ùˆ Ø§Ø¬ØªÙ‡Ø§Ø¯ Ø°Ùˆ ØµÙ„Ø© Ø¬Ø²Ø¦ÙŠØ©
- 0-4: Ù…Ø§Ø¯Ø© Ù…Ù† Ù‚Ø§Ù†ÙˆÙ† Ø¢Ø®Ø± Ø£Ùˆ Ù…ÙˆØ¶ÙˆØ¹ Ù…Ø®ØªÙ„Ù

Ù…Ø«Ø§Ù„: Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ø³Ø¤Ø§Ù„ Ø¹Ù† "Ø§Ù„Ø³Ø±Ù‚Ø© Ø¨Ø§Ù„Ø¹Ù†Ù":
- Ø§Ù„Ù…Ø§Ø¯Ø© 350 Ù…ÙƒØ±Ø± (Ø§Ù„Ø³Ø±Ù‚Ø© Ù…Ø¹ Ø§Ù„Ø¹Ù†Ù) = 10
- Ù‚Ø±Ø§Ø± Ø§Ù„Ù…Ø­ÙƒÙ…Ø© Ø§Ù„Ø¹Ù„ÙŠØ§ Ø­ÙˆÙ„ Ø¸Ø±Ù Ø§Ù„Ø¹Ù†Ù = 9
- Ø§Ù„Ù…Ø§Ø¯Ø© 351 (Ø§Ù„Ø³Ø±Ù‚Ø© Ø§Ù„Ù…Ø´Ø¯Ø¯Ø©) = 9
- Ø§Ù„Ù…Ø§Ø¯Ø© 388 (Ø¥Ø®ÙØ§Ø¡ Ø§Ù„Ø£Ø´ÙŠØ§Ø¡) = 5
- Ù‚Ø§Ù†ÙˆÙ† Ø§Ù„ÙƒÙ‡Ø±Ø¨Ø§Ø¡ = 0

Ø£Ø¬Ø¨ Ø¨Ù€ JSON ÙÙ‚Ø·: {{"1": 8, "2": 5, ...}}"""
    
    # OPENROUTER: Use light model for reranking to save cost/time
    try:
        response = generate_openrouter(prompt, model="google/gemini-2.0-flash-001")
        import json
        json_match = re.search(r'\{[^}]+\}', response.text)
        if json_match:
            scores = json.loads(json_match.group())
            ranked = []
            for i, chunk in enumerate(chunks[:10], 1):
                score = float(scores.get(str(i), 0)) / 10.0
                ranked.append((chunk, score))
            for chunk in chunks[10:]:
                ranked.append((chunk, 0.1))
            ranked.sort(key=lambda x: x[1], reverse=True)
            return ranked[:top_k]
        return [(chunk, 0.5) for chunk in chunks[:top_k]]
    except Exception as e:
        # Avoid printing full exception if it contains Arabic
        return [(chunk, 0.5) for chunk in chunks[:top_k]]

def generate_gemini_flash(prompt: str):
    """
    Dedicated function for Generation using Gemini Flash Latest (via REST API).
    Used for Consult and Pleading modes to ensure high-quality reasoning.
    Refactored from SDK to REST API to reduce bundle size for Vercel.
    """
    try:
        if not settings.GEMINI_API_KEY:
             raise ValueError("GEMINI_API_KEY not set")
        
        # Use the confirmed working model via REST API
        MODEL_NAME = "gemini-2.0-flash"
        url = f"https://generativelanguage.googleapis.com/v1beta/models/{MODEL_NAME}:generateContent?key={settings.GEMINI_API_KEY}"
        
        headers = {"Content-Type": "application/json"}
        payload = {
            "contents": [{"parts": [{"text": prompt}]}],
            "generationConfig": {
                "temperature": 0.3, # Low temp for precise legal reasoning
                "maxOutputTokens": 8192
            }
        }
        
        resp = requests.post(url, headers=headers, json=payload, timeout=120)
        
        if resp.status_code == 200:
            result = resp.json()
            try:
                text = result['candidates'][0]['content']['parts'][0]['text']
                return GenerationResponse(text=text)
            except (KeyError, IndexError):
                print(f"[Gemini Flash] Bad Response format: {result}")
                raise ValueError("Gemini Flash response parsing failed")
        else:
            print(f"[Gemini Flash] Error {resp.status_code}: {resp.text[:500]}")
            raise Exception(f"Gemini Flash API Error: {resp.status_code}")
        
    except Exception as e:
        print(f"[Gemini Flash] REST Error: {e}")
        print("[Gemini Flash] Falling back to Groq/standard API...")
        return generate_with_retry(None, prompt)

def generate_openrouter(prompt: str, model: str = None):
    """
    Generate text using OpenRouter API.
    Args:
        prompt: The input prompt
        model: Optional model override. If None, uses OPENROUTER_MODEL env var (Gemini 3).
    """
    try:
        api_key = settings.OPENROUTER_API_KEY
        # Default to heavy model (Gemini 3) if not specified
        target_model = model or getattr(settings, 'OPENROUTER_MODEL', "google/gemini-2.0-flash-001")
        
        if not api_key:
            print("[OpenRouter] No API Key set, skipping to fallback (Gemini Flash Direct)...")
            return generate_gemini_flash(prompt)

        url = "https://openrouter.ai/api/v1/chat/completions"
        headers = {
            "Authorization": f"Bearer {api_key}",
            "HTTP-Referer": "https://quanouni.ai", 
            "X-Title": "Qanouni AI",
            "Content-Type": "application/json"
        }
        data = {
            "model": target_model,
            "messages": [{"role": "user", "content": prompt}],
            "temperature": 0.3,
            "max_tokens": 64000 # Support large logic
        }
        
        # print(f"[OpenRouter] Requesting model: {model}...")
        resp = requests.post(url, headers=headers, json=data, timeout=120)
        
        if resp.status_code == 200:
            result = resp.json()
            if 'choices' in result and len(result['choices']) > 0:
                content = result['choices'][0]['message']['content']
                return GenerationResponse(text=content)
            else:
                 print(f"[OpenRouter] Empty choices: {result}")
                 return generate_gemini_flash(prompt)
        else:
            print(f"[OpenRouter] Error {resp.status_code}: {resp.text[:500]}")
            # Fallback
            print("[OpenRouter] Falling back to Gemini Flash Direct...")
            return generate_gemini_flash(prompt)
            
    except Exception as e:
        print(f"[OpenRouter] Exception: {e}")
        return generate_gemini_flash(prompt)

class RAGService:
    def __init__(self):
        # No SDK configuration needed.
        self.model = None # We don't use the SDK model object anymore

    def _retrieve(self, query, filters=None, top_k=20):
        # ... (unchanged) ...
        try:
            query_embedding = get_embedding(query, is_query=True)
            vector_results = query_chroma(query_embedding, n_results=top_k, where=filters)
            v_docs = vector_results['documents'][0] if vector_results and 'documents' in vector_results else []
            v_metas = vector_results['metadatas'][0] if vector_results and 'metadatas' in vector_results else []
        except Exception as e:
            print(f"Vector search failed")
            v_docs, v_metas = [], []

        # 2. BM25 Search
        from app.services.bm25_service import bm25_service
        bm25_results = bm25_service.search(query, top_k=top_k, filters=filters)

        # 3. RRF Fusion (BM25-heavy due to poor vector search for Arabic legal text)
        k = 60
        scores = {}
        meta_map = {}
        
        # Combine (BM25 prioritized because vector similarity is weak for Arabic)
        for r, d in enumerate(v_docs):
            scores[d] = scores.get(d, 0) + (0.3 / (k + r + 1))  # Vector: 30%
            if r < len(v_metas): meta_map[d] = v_metas[r]
            
        for r, (d, s, m) in enumerate(bm25_results):
            scores[d] = scores.get(d, 0) + (0.7 / (k + r + 1))  # BM25: 70%
            meta_map[d] = m

        ranked_docs = sorted(scores.items(), key=lambda x: x[1], reverse=True)
        final_docs = [d for d, s in ranked_docs[:15]]
        final_metas = [meta_map.get(d, {}) for d in final_docs]
        
        return final_docs, final_metas

    def answer_query(self, query: str, filters: dict = None, skip_generation: bool = False):
        # Standard Research Mode
        docs, metas = self._retrieve(query, filters)
        
        # Rerank
        if not skip_generation:
            reranked = rerank_with_gemini(query, docs, top_k=5)
            # Re-map metadata
            final_docs = [r[0] for r in reranked]
            final_metas = []
            for d in final_docs:
                try: 
                    idx = docs.index(d)
                    final_metas.append(metas[idx])
                except: final_metas.append({})
        else:
            final_docs, final_metas = docs[:5], metas[:5]

        # Context formatting
        context = ""
        for i, (doc, meta) in enumerate(zip(final_docs, final_metas), 1):
            title = meta.get('filename', f'Source {i}').replace('.txt', '')
            context += f"\n\n### [Ù…ØµØ¯Ø± {i}: {title}]\n{doc}\n"

        if skip_generation:
            return {"answer": "Retrieval Only", "context": final_docs, "metadatas": final_metas}

        # Prompt - Professional Legal Research (v2.0)
        prompt = f"""Ø£Ù†Øª **Ø¨Ø§Ø­Ø« Ù‚Ø§Ù†ÙˆÙ†ÙŠ Ù…ØªØ®ØµØµ ÙÙŠ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ† Ø§Ù„Ø¬Ø²Ø§Ø¦Ø±ÙŠ**ØŒ ØªØ¹Ù…Ù„ ÙÙŠ Ù…ÙƒØªØ¨Ø© Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© Ø£ÙƒØ§Ø¯ÙŠÙ…ÙŠØ©.
Ù…Ø±Ø¬Ø¹ÙŠØªÙƒ Ø§Ù„Ø­ØµØ±ÙŠØ© Ù‡ÙŠ Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© Ø§Ù„Ù…Ù‚Ø¯Ù…Ø© Ø£Ø¯Ù†Ø§Ù‡ ÙÙ‚Ø·.

## Ù…Ù‡Ù…ØªÙƒ
ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø³Ø¤Ø§Ù„ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠ ÙˆØªÙ‚Ø¯ÙŠÙ… Ø¥Ø¬Ø§Ø¨Ø© Ø¯Ù‚ÙŠÙ‚Ø© ÙˆÙ…Ø¨Ø±Ø±Ø© Ø¨Ø§Ù„Ù†ØµÙˆØµ.

## Ù‚ÙˆØ§Ø¹Ø¯ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø§Ù„Ø¥Ù„Ø²Ø§Ù…ÙŠØ©:
1. **Ù„Ø§ ØªØ®ØªÙ„Ù‚ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª**: Ø¥Ø°Ø§ Ù„Ù… ØªØ¬Ø¯ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© ÙÙŠ Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ù…Ù‚Ø¯Ù…Ø©ØŒ Ù‚Ù„ Ø°Ù„Ùƒ ØµØ±Ø§Ø­Ø©: "Ù„Ù… Ø£Ø¬Ø¯ Ù†ØµØ§Ù‹ ØµØ±ÙŠØ­Ø§Ù‹ ÙŠØ¬ÙŠØ¨ Ø¹Ù„Ù‰ Ù‡Ø°Ø§ Ø§Ù„Ø³Ø¤Ø§Ù„ ÙÙŠ Ø§Ù„Ù…ØµØ§Ø¯Ø± Ø§Ù„Ù…ØªØ§Ø­Ø©."
2. **Ø§Ù„Ø§Ù‚ØªØ¨Ø§Ø³ Ø§Ù„Ø¯Ù‚ÙŠÙ‚**: Ø¹Ù†Ø¯ Ø§Ù„Ø§Ø³ØªØ´Ù‡Ø§Ø¯ Ø¨Ù…Ø§Ø¯Ø© Ù‚Ø§Ù†ÙˆÙ†ÙŠØ©ØŒ Ø§Ù‚ØªØ¨Ø³ Ù†ØµÙ‡Ø§ Ø§Ù„Ø­Ø±ÙÙŠ Ø¨ÙŠÙ† Ø¹Ù„Ø§Ù…Ø§Øª ØªÙ†ØµÙŠØµ Â« Â».
3. **Ø§Ù„Ø¥Ø´Ø§Ø±Ø© Ù„Ù„Ù…ØµØ§Ø¯Ø±**: Ø§Ø³ØªØ®Ø¯Ù… Ø£Ø±Ù‚Ø§Ù… Ø§Ù„Ù…ØµØ§Ø¯Ø± [1], [2], [3] Ø¨Ø¹Ø¯ ÙƒÙ„ Ø§Ù‚ØªØ¨Ø§Ø³ Ø£Ùˆ Ù…Ø¹Ù„ÙˆÙ…Ø©.
4. **Ø§Ù„ØªÙ†Ø³ÙŠÙ‚ Ø§Ù„Ø¥Ù„Ø²Ø§Ù…ÙŠ**:
   - Ø§Ø¨Ø¯Ø£ Ø¨Ù€ **## Ù…Ù„Ø®Øµ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©** (3 Ø£Ø³Ø·Ø± ÙƒØ­Ø¯ Ø£Ù‚ØµÙ‰)
   - Ø«Ù… **## Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠ** Ù…Ø¹ Ø´Ø±Ø­ Ù…ÙØµÙ„ ÙˆØ£Ø±Ù‚Ø§Ù… Ø§Ù„Ù…ØµØ§Ø¯Ø±
   - Ø§Ø®ØªÙ… Ø¨Ù€ **## Ø§Ù„Ù…Ø±Ø§Ø¬Ø¹** (Ù‚Ø§Ø¦Ù…Ø© Ù…Ø±Ù‚Ù…Ø©: Ø§Ø³Ù… Ø§Ù„Ù‚Ø§Ù†ÙˆÙ† + Ø±Ù‚Ù… Ø§Ù„Ù…Ø§Ø¯Ø© Ø¥Ù† ÙˆØ¬Ø¯)
5. **Ø§Ù„Ù„ØºØ©**: Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø§Ù„ÙØµØ­Ù‰ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© Ø§Ù„Ø±Ø³Ù…ÙŠØ©. Ù„Ø§ ØªØ³ØªØ®Ø¯Ù… Ø§Ù„Ø¹Ø§Ù…ÙŠØ© Ø£Ø¨Ø¯Ø§Ù‹.
6. **Ø§Ù„Ù…ÙˆØ¶ÙˆØ¹ÙŠØ©**: Ù„Ø§ ØªÙØ¨Ø¯Ù Ø±Ø£ÙŠØ§Ù‹ Ø´Ø®ØµÙŠØ§Ù‹. Ø§Ù„ØªØ²Ù… Ø¨Ù…Ø§ Ø¬Ø§Ø¡ ÙÙŠ Ø§Ù„Ù†ØµÙˆØµ.

## Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© Ø§Ù„Ù…ØªØ§Ø­Ø©:
{context}

## Ø§Ù„Ø³Ø¤Ø§Ù„:
{query}

## Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©:"""

        try:
            # OPENROUTER: Use light model for General Search (interactive speed)
            response = generate_openrouter(prompt, model="google/gemini-2.0-flash-001")
            # response = generate_with_retry(self.model, prompt)
            answer = response.text.replace('"]', '"]\n') # Hack for ref formatting
        except Exception as e:
            print(f"Generation failed after retries: {e}")
            answer = "Ø¹Ø°Ø±Ø§Ù‹ØŒ Ø§Ù„Ù†Ø¸Ø§Ù… Ù…Ø´ØºÙˆÙ„ Ø¬Ø¯Ø§Ù‹ Ø­Ø§Ù„ÙŠØ§Ù‹ (Ø¶ØºØ· Ø¹Ù„Ù‰ Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„). Ù‡Ø°Ù‡ Ù‡ÙŠ Ø§Ù„Ù…ØµØ§Ø¯Ø± Ø§Ù„ØªÙŠ ÙˆØ¬Ø¯ØªÙ‡Ø§ØŒ Ù„ÙƒÙ† Ù„Ù… Ø£ØªÙ…ÙƒÙ† Ù…Ù† ØµÙŠØ§ØºØ© Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©. ÙŠØ±Ø¬Ù‰ Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© Ø¨Ø¹Ø¯ Ù‚Ù„ÙŠÙ„."
        
        return {
            "query": query, 
            "answer": answer, 
            "sources": [{
                "filename": m.get('filename'),
                "document_id": m.get('document_id'),
                "chunk_index": m.get('chunk_index', i+1),
                "content_preview": final_docs[i][:150] + "..." if len(final_docs[i]) > 150 else final_docs[i]
            } for i, m in enumerate(final_metas)]
        }

    def _extract_search_query(self, situation: str) -> str:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø°ÙƒÙŠ Ù„Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… LLM Ù„ØªØ­Ø³ÙŠÙ† Ø¯Ù‚Ø© Ø§Ù„Ø¨Ø­Ø«"""
        try:
            prompt = f"""Ø£Ù†Øª Ø®Ø¨ÙŠØ± Ù‚Ø§Ù†ÙˆÙ†ÙŠ Ø°ÙƒÙŠ. Ù…Ù‡Ù…ØªÙƒ Ù‡ÙŠ ØªØ­Ù„ÙŠÙ„ Ù…ÙˆÙ‚Ù Ù‚Ø§Ù†ÙˆÙ†ÙŠ ÙˆØ§Ø³ØªØ®Ø±Ø§Ø¬ Ø£ÙØ¶Ù„ ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ø¨Ø­Ø« Ù„Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø§Ù„Ù‚ÙˆØ§Ù†ÙŠÙ† ÙˆØ§Ù„Ù…Ø±Ø§Ø¬Ø¹ Ø§Ù„Ù…Ù†Ø§Ø³Ø¨Ø©.
            
Ø§Ù„Ù…ÙˆÙ‚Ù:
{situation[:2000]}

Ø§Ø³ØªØ®Ø±Ø¬ Ù…Ø§ ÙŠÙ„ÙŠ ÙÙŠ Ø³Ø·Ø± ÙˆØ§Ø­Ø¯ ÙÙ‚Ø·:
1. Ù†ÙˆØ¹ Ø§Ù„Ù‚Ø¶ÙŠØ© (Ù…Ø«Ù„Ø§Ù‹: Ù…ÙŠØ±Ø§Ø«ØŒ Ø¹Ù…Ù„ØŒ Ø¬Ù†Ø§Ø¦ÙŠØŒ Ø£Ø­ÙˆØ§Ù„ Ø´Ø®ØµÙŠØ©)
2. 5 Ø¥Ù„Ù‰ 10 ÙƒÙ„Ù…Ø§Øª Ù…ÙØªØ§Ø­ÙŠØ© Ø¯Ù‚ÙŠÙ‚Ø© Ù„Ù„Ø¨Ø­Ø« ÙÙŠ Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© (Ù…Ø«Ù„Ø§Ù‹: Ø§Ø³ØªØ®Ø¯Ù… "ØªØ³Ø±ÙŠØ­ ØªØ¹Ø³ÙÙŠ" Ø¨Ø¯Ù„Ø§Ù‹ Ù…Ù† "Ø·Ø±Ø¯"ØŒ "Ù‚Ø³Ù…Ø© ØªØ±ÙƒØ©" Ø¨Ø¯Ù„Ø§Ù‹ Ù…Ù† "ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ø¥Ø±Ø«")

ØªÙ†Ø³ÙŠÙ‚ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨:
[Ù†ÙˆØ¹ Ø§Ù„Ù‚Ø¶ÙŠØ©] ÙƒÙ„Ù…Ø§Øª Ù…ÙØªØ§Ø­ÙŠØ©

Ø£Ø¬Ø¨ ÙÙ‚Ø· Ø¨Ø§Ù„Ø³Ø·Ø± Ø§Ù„Ù…Ø·Ù„ÙˆØ¨ Ø¨Ø¯ÙˆÙ† Ø£ÙŠ Ù…Ù‚Ø¯Ù…Ø§Øª Ø£Ùˆ Ø´Ø±Ø­."""

            # Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„ Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ©
            # Ù†Ø³ØªØ®Ø¯Ù… OpenRouter (Gemini 2 Filter) Ù„Ù„Ø³Ø±Ø¹Ø© ÙˆØ§Ù„Ø¯Ù‚Ø© ÙˆØ§Ù„ØªÙƒÙ„ÙØ©
            # OPENROUTER: Use light model for extraction
            response = generate_openrouter(prompt, model="google/gemini-2.0-flash-001")
            
            if response and hasattr(response, 'text'):
                extracted_text = response.text.strip()
                print(f"[Smart Extract] LLM Output: {extracted_text}")
                
                # Ø¯Ù…Ø¬ ÙˆØµÙ Ø§Ù„Ù…ÙˆÙ‚Ù (Ø£ÙˆÙ„ 50 ÙƒÙ„Ù…Ø© Ù„Ù„Ø³ÙŠØ§Ù‚) Ù…Ø¹ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…Ø³ØªØ®Ø±Ø¬Ø© Ø°ÙƒÙŠØ§Ù‹
                # Ù‡Ø°Ø§ ÙŠØ¶Ù…Ù† ÙˆØ¬ÙˆØ¯ Ø§Ù„Ø³ÙŠØ§Ù‚ Ø§Ù„Ø£ØµÙ„ÙŠ + Ø§Ù„Ù…ØµØ·Ù„Ø­Ø§Øª Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© Ø§Ù„Ø¯Ù‚ÙŠÙ‚Ø©
                words = situation.split()[:50]
                combined_query = " ".join(words) + " " + extracted_text
                return combined_query
            
            print("[Smart Extract] Empty response, falling back.")
            return " ".join(situation.split()[:80])

        except Exception as e:
            print(f"[Smart Extract] Error: {e}")
            # Fallback to simple truncation
            return " ".join(situation.split()[:80])

    def consult(self, situation: str):
        """ÙˆØ¶Ø¹ Ø§Ù„Ù…Ø³ØªØ´Ø§Ø± Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠ - Ø§Ø³ØªØ´Ø§Ø±Ø© Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© Ø§Ø­ØªØ±Ø§ÙÙŠØ©"""
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ø³ØªØ¹Ù„Ø§Ù… Ø¨Ø­Ø« Ù…Ø±ÙƒØ² Ù…Ù† Ø§Ù„Ù…ÙˆÙ‚Ù
        search_query = self._extract_search_query(situation)
        
        # Search for relevant laws AND jurisprudence using focused query
        # UPGRADE: Fetch 50 docs for Gemini 3 massive context
        docs, metas = self._retrieve(search_query, top_k=50)
        
        # Rerank using original situation for context relevance
        # RE-ENABLED: Using Gemini Flash (Fast) to filter irrelevant jurisprudence effectively
        try:
            reranked = rerank_with_gemini(situation, docs, top_k=3)
            final_docs = [r[0] for r in reranked]
            final_metas = []
            doc_map = {d: m for d, m in zip(docs, metas)}
            for d in final_docs: final_metas.append(doc_map.get(d, {}))
        except Exception:
            # Fallback if reranker fails
            final_docs = docs[:20]
            final_metas = metas[:20]
        
        # Format context with source type indication (full text like Legal Search)
        context = ""
        for i, (doc, meta) in enumerate(zip(final_docs, final_metas), 1):
            source_name = meta.get('filename', f'Ù…ØµØ¯Ø± {i}').replace('.txt', '')
            # Clean Cyrillic or bad chars in titles (OCR artifacts)
            # Robust fix for "Ğ½Ğ°" (Cyrillic/Latin mix)
            # Replace specifically the known bad pattern or generic non-arabic in Arabic context
            source_name = re.sub(r'[\u0400-\u04FF]+', 'Ø¹Ù„Ù‰', source_name) # Replace Cyrillic with 'Ø¹Ù„Ù‰' (heuristic)
            if 'Ğ½Ğ°' in source_name or 'ha' in source_name: 
                 source_name = source_name.replace(' Ğ½Ğ° ', ' Ø¹Ù„Ù‰ ').replace(' ha ', ' Ø¹Ù„Ù‰ ')
            source_name = source_name.replace('_', ' ')
            
            source_type = "Ø§Ø¬ØªÙ‡Ø§Ø¯ Ù‚Ø¶Ø§Ø¦ÙŠ" if "Ù‚Ø±Ø§Ø±" in source_name or "Ø§Ø¬ØªÙ‡Ø§Ø¯" in source_name else "Ù†Øµ Ù‚Ø§Ù†ÙˆÙ†ÙŠ"
            context += f"\n\n### [{source_type} - Ù…ØµØ¯Ø± {i}: {source_name}]\n{doc}\n"
        
        # Professional Legal Consultant Prompt (v2.2 - Explicit Citations)
        prompt = f"""Ø£Ù†Øª **Ù…Ø­Ø§Ù…Ù Ø£ÙˆÙ„ Ù…Ø¹ØªÙ…Ø¯ Ù„Ø¯Ù‰ Ø§Ù„Ù…Ø­ÙƒÙ…Ø© Ø§Ù„Ø¹Ù„ÙŠØ§ Ø§Ù„Ø¬Ø²Ø§Ø¦Ø±ÙŠØ©**.
Ù…Ù‡Ù…ØªÙƒ ØªÙ‚Ø¯ÙŠÙ… Ø§Ø³ØªØ´Ø§Ø±Ø© Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© Ø¯Ù‚ÙŠÙ‚Ø© Ø¨Ù†Ø§Ø¡Ù‹ *Ø­ØµØ±Ø§Ù‹* Ø¹Ù„Ù‰ Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ù…Ù‚Ø¯Ù…Ø©.

## Ø§Ù„Ù…ÙˆÙ‚Ù Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠ:
{situation}

## Ø§Ù„Ù…ØµØ§Ø¯Ø± Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© Ø§Ù„Ù…ØªØ§Ø­Ø©:
{context}

âš ï¸ **ØªØ¹Ù„ÙŠÙ…Ø§Øª ØµØ§Ø±Ù…Ø© Ù„Ù„Ø§Ø³ØªØ´Ù‡Ø§Ø¯**:
1. **Ù„Ø§ ØªÙ‚Ù„ Ø£Ø¨Ø¯Ø§Ù‹ "Ù…ØµØ¯Ø± 1" Ø£Ùˆ "Source 1"**.
2. **Ø§Ø³ØªØ®Ø¯Ù… Ø§Ø³Ù… Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†/Ø§Ù„Ù…Ø±Ø¬Ø¹** Ø§Ù„Ù…Ø°ÙƒÙˆØ± ÙÙŠ Ø¹Ù†ÙˆØ§Ù† Ø§Ù„Ù…ØµØ¯Ø±.
   - âŒ Ø®Ø·Ø£: "ØªÙ†Øµ Ø§Ù„Ù…Ø§Ø¯Ø© 8 Ù…Ù† Ù…ØµØ¯Ø± 1..."
   - âœ… ØµØ­: "ØªÙ†Øµ Ø§Ù„Ù…Ø§Ø¯Ø© 8 Ù…Ù† **Ø£Ù…Ø± Ø¥Ù„Ø²Ø§Ù…ÙŠØ© Ø§Ù„ØªØ£Ù…ÙŠÙ† Ø¹Ù„Ù‰ Ø§Ù„Ø³ÙŠØ§Ø±Ø§Øª**..."
3. ØµØ­Ø­ Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„Ù‚ÙˆØ§Ù†ÙŠÙ† Ø¥Ø°Ø§ Ø¸Ù‡Ø±Øª Ø¨Ù‡Ø§ Ø£Ø®Ø·Ø§Ø¡ Ø¥Ù…Ù„Ø§Ø¦ÙŠØ© (Ù…Ø«Ù„Ø§Ù‹: "Ğ½Ğ°" -> "Ø¹Ù„Ù‰").

## Ù‡ÙŠÙƒÙ„Ø© Ø§Ù„Ø§Ø³ØªØ´Ø§Ø±Ø© (Ø§Ù„ØªØ²Ù… Ø¨Ù‡Ø°Ø§ Ø§Ù„ØªØ±ØªÙŠØ¨):

### 1. Ø§Ù„ØªÙƒÙŠÙŠÙ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠ âš–ï¸
- Ù…Ø§ Ù‡Ùˆ Ù†ÙˆØ¹ Ø§Ù„Ù‚Ø¶ÙŠØ©ØŸ (Ù…Ø¯Ù†ÙŠ / Ø¬Ø²Ø§Ø¦ÙŠ / Ø¥Ø¯Ø§Ø±ÙŠ / Ø£Ø³Ø±Ø© / Ø¹Ù…Ù„ / ØªØ¬Ø§Ø±ÙŠ)
- Ù…Ø§ Ù‡ÙŠ Ø§Ù„ÙˆÙ‚Ø§Ø¦Ø¹ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© Ø§Ù„Ù…Ø¤Ø«Ø±Ø©ØŸ
- Ù…Ù† Ù‡Ù… Ø§Ù„Ø£Ø·Ø±Ø§Ù ÙˆÙ…Ø§ ØµÙØ§ØªÙ‡Ù… Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ©ØŸ

### 2. Ø§Ù„Ø£Ø³Ø§Ø³ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠ ğŸ“š
- **Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ©**: Ø§Ø°ÙƒØ± Ø§Ù„Ù…ÙˆØ§Ø¯ Ø§Ù„Ù…Ù†Ø·Ø¨Ù‚Ø© Ù…Ø¹ Ù†ØµÙ‡Ø§ Ø¨ÙŠÙ† Â« Â»ØŒ ÙˆÙ†Ø³Ø¨ØªÙ‡Ø§ Ø¥Ù„Ù‰ **Ø§Ø³Ù… Ø§Ù„Ù‚Ø§Ù†ÙˆÙ† Ø§Ù„ØµØ±ÙŠØ­**.
- **Ø§Ù„Ø§Ø¬ØªÙ‡Ø§Ø¯ Ø§Ù„Ù‚Ø¶Ø§Ø¦ÙŠ**: Ø¥Ù† ÙˆØ¬Ø¯ Ù‚Ø±Ø§Ø± Ù…Ù† Ø§Ù„Ù…Ø­ÙƒÙ…Ø© Ø§Ù„Ø¹Ù„ÙŠØ§ØŒ Ø§Ø°ÙƒØ±Ù‡ Ø¨Ø±Ù‚Ù… Ø§Ù„Ù‚Ø±Ø§Ø± ÙˆØ§Ù„Ø³Ù†Ø©.

### 3. Ø§Ù„ØªÙˆØ¬ÙŠÙ‡ Ø§Ù„Ø¹Ù…Ù„ÙŠ ğŸ¯
- Ù…Ø§ Ø§Ù„Ø¥Ø¬Ø±Ø§Ø¡ Ø§Ù„ÙˆØ§Ø¬Ø¨ØŸ (Ø´ÙƒÙˆÙ‰ / Ø¯Ø¹ÙˆÙ‰ / ØµÙ„Ø­ / ØªØ¸Ù„Ù… / Ø§Ø³ØªØ¦Ù†Ø§Ù)
- Ø£Ù…Ø§Ù… Ø£ÙŠ Ø¬Ù‡Ø©ØŸ (Ù…Ø­ÙƒÙ…Ø© Ø§Ø¨ØªØ¯Ø§Ø¦ÙŠØ© / Ù…Ø¬Ù„Ø³ Ù‚Ø¶Ø§Ø¦ÙŠ / Ù…Ø­ÙƒÙ…Ø© Ø¹Ù„ÙŠØ§ / Ø¥Ø¯Ø§Ø±Ø©)
- Ù…Ø§ Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚ ÙˆØ§Ù„Ø£Ø¯Ù„Ø© Ø§Ù„Ù„Ø§Ø²Ù…Ø©ØŸ

### 4. Ø§Ù„ØªØ­Ø°ÙŠØ±Ø§Øª Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© âš ï¸
- Ø¢Ø¬Ø§Ù„ Ø§Ù„ØªÙ‚Ø§Ø¯Ù… Ø£Ùˆ Ø§Ù„Ø³Ù‚ÙˆØ· (Ø¥Ù† ÙˆØ¬Ø¯Øª)
- Ø§Ù„Ù…Ø®Ø§Ø·Ø± Ø§Ù„Ù…Ø­ØªÙ…Ù„Ø© Ø¥Ù† Ù„Ù… ÙŠÙØªØ®Ø° Ø¥Ø¬Ø±Ø§Ø¡ Ø³Ø±ÙŠØ¹

### 5. Ø§Ù„Ø®Ù„Ø§ØµØ© ÙˆØ§Ù„ØªÙˆØµÙŠØ© ğŸ“Œ
- Ù…Ù„Ø®Øµ Ø§Ù„Ù…ÙˆÙ‚Ù ÙÙŠ 3 Ø£Ø³Ø·Ø±
- Ø§Ù„ØªÙˆØµÙŠØ© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ© Ø§Ù„ÙˆØ§Ø¶Ø­Ø©

---
âš ï¸ **ØªÙ†ÙˆÙŠÙ‡**: Ù‡Ø°Ù‡ Ø§Ø³ØªØ´Ø§Ø±Ø© Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© Ø£ÙˆÙ„ÙŠØ© Ù…Ø¨Ù†ÙŠØ© Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù…Ù‚Ø¯Ù…Ø©. ÙŠÙÙ†ØµØ­ Ø¨Ù…Ø±Ø§Ø¬Ø¹Ø© Ù…Ø­Ø§Ù…Ù Ù…ØªØ®ØµØµ Ù„Ø¯Ø±Ø§Ø³Ø© Ù…Ù„Ù Ø§Ù„Ù‚Ø¶ÙŠØ© ÙƒØ§Ù…Ù„Ø§Ù‹."""

        try:
            # UPGRADE: Use OpenRouter (Gemini 3) for superior reasoning
            print(f"[Consult] Using OpenRouter (Gemini 3) for superior reasoning...")
            response = generate_openrouter(prompt)
            consultation_text = response.text
        except Exception as e:
            print(f"Consultation generation failed: {e}")
            consultation_text = "Ø¹Ø°Ø±Ø§Ù‹ØŒ Ù„Ù… Ø£ØªÙ…ÙƒÙ† Ù…Ù† ØµÙŠØ§ØºØ© Ø§Ù„Ø§Ø³ØªØ´Ø§Ø±Ø© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ© Ø¨Ø³Ø¨Ø¨ Ø¶ØºØ· Ø§Ù„Ù†Ø¸Ø§Ù…. ÙŠØ±Ø¬Ù‰ Ù…Ø±Ø§Ø¬Ø¹Ø© Ø§Ù„Ù…ØµØ§Ø¯Ø± Ø£Ø¯Ù†Ø§Ù‡."

        # Build improved source titles
        sources_list = []
        for d, m in zip(final_docs, final_metas):
            title = m.get('filename', 'Source').replace('.txt', '')
            # Clean Cyrillic or bad chars in titles (OCR artifacts) - APPLY TO OUTPUT LIST TOO
            title = re.sub(r'[\u0400-\u04FF]+', 'Ø¹Ù„Ù‰', title)
            if 'Ğ½Ğ°' in title or 'ha' in title:
                 title = title.replace(' Ğ½Ğ° ', ' Ø¹Ù„Ù‰ ').replace(' ha ', ' Ø¹Ù„Ù‰ ')
            title = title.replace('_', ' ')
            
            # 1. Try Metadata Article Number
            if m.get('article_number'):
                title = f"{title} - Ø§Ù„Ù…Ø§Ø¯Ø© {m['article_number']}"
            
            # 2. If Jurisprudence, try to extract decision number or date from content
            elif 'Ù‚Ø±Ø§Ø±' in d[:100] or 'ØªØ³Ø±ÙŠØ­' in title:
                # Try to find "Ù‚Ø±Ø§Ø± Ø±Ù‚Ù… X"
                match_decision = re.search(r'Ù‚Ø±Ø§Ø±\s+Ø±Ù‚Ù…\s*[:\s]\s*(\d+)', d)
                if match_decision:
                    title = f"Ù‚Ø±Ø§Ø± Ø§Ù„Ù…Ø­ÙƒÙ…Ø© Ø§Ù„Ø¹Ù„ÙŠØ§ Ø±Ù‚Ù… {match_decision.group(1)} ({title})"
                else:
                    # Fallback: Try to find article reference in text
                    match_art = re.search(r'Ø§Ù„Ù…Ø§Ø¯Ø©\s+(\d+)', d)
                    if match_art:
                        title = f"{title} (Ø¥Ø´Ø§Ø±Ø© Ù„Ù„Ù…Ø§Ø¯Ø© {match_art.group(1)})"

            # 3. Fallback to law name
            elif m.get('law_name'): 
                law = m['law_name'].replace('.txt', '')
                if law != title:
                    title = f"{law} ({title})"
            
            sources_list.append({
                "title": title, 
                "content": d,
                "document_id": m.get('document_id'),
                "chunk_index": m.get('chunk_index'),
                "filename": m.get('filename'),
                "metadata": m
            })

        return {
            "answer": consultation_text,
            "sources": sources_list
        }

    def draft_pleading(self, case_data: dict, pleading_type="Ø¯ÙØ§Ø¹", style="formel", top_k=30):
        """
        ÙˆØ¶Ø¹ Ø§Ù„Ù…Ø­Ø§Ù…ÙŠ: ØªÙˆÙ„ÙŠØ¯ Ù…Ø°ÙƒØ±Ø§Øª Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© Ø§Ø­ØªØ±Ø§ÙÙŠØ© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù‡ÙŠÙƒÙ„ÙŠØ© Ø§Ù„Ù…Ø±Ø§ÙØ¹Ø§Øª Ø§Ù„Ø°Ù‡Ø¨ÙŠØ©
        Advocate Mode: Generate professional legal pleadings
        Enhanced v2.1: Few-Shot + Interactive Sources + OCR Cleaning
        """
        facts = case_data.get('facts', '')
        charges = " ".join(case_data.get('charges', []))
        defendant_name = case_data.get('defendant_name', 'Ø§Ù„Ù…ØªÙ‡Ù…')
        court = case_data.get('court', 'Ø§Ù„Ù…Ø­ÙƒÙ…Ø© Ø§Ù„Ù…Ø®ØªØµØ©')
        case_number = case_data.get('case_number', '')
        
        # Extract defense strategy if available
        defense_strategy = case_data.get('defense_strategy', {})
        main_defense = defense_strategy.get('main_argument', '')
        secondary_args = defense_strategy.get('secondary_arguments', [])
        
        # 1. Smart Extraction from Case Data
        case_context = f"Ø§Ù„ØªÙ‡Ù…Ø©: {charges}. Ø§Ù„ÙˆÙ‚Ø§Ø¦Ø¹: {facts}"
        search_query = self._extract_search_query(case_context)
        print(f"[Pleading] Smart Query: {search_query}")

        # 2. Retrieval
        # 2. Retrieval - UPGRADE: Fetch more docs for Gemini 3 Flash Large Context
        docs, metas = self._retrieve(search_query, top_k=60) # Increased from default/30 to 60 for large context
        
        # 3. Reranking using Gemini
        # 3. Reranking using Gemini - KEEP TOP 20 INSTEAD OF 5
        reranked = rerank_with_gemini(case_context, docs, top_k=20)
        final_docs = [r[0] for r in reranked]
        final_metas = []
        
        doc_map = {d: m for d, m in zip(docs, metas)}
        for d in final_docs: final_metas.append(doc_map.get(d, {}))
        
        # 4. Build Legal Context with CLEAN source names (TRUNCATED to avoid timeout)
        context = ""
        for i, (doc, meta) in enumerate(zip(final_docs, final_metas), 1):
            source_name = meta.get('filename', f'Ù…ØµØ¯Ø± {i}').replace('.txt', '')
            # Clean Cyrillic OCR artifacts
            source_name = re.sub(r'[\u0400-\u04FF]+', 'Ø¹Ù„Ù‰', source_name)
            source_name = source_name.replace(' Ğ½Ğ° ', ' Ø¹Ù„Ù‰ ').replace(' ha ', ' Ø¹Ù„Ù‰ ')
            source_name = source_name.replace('_', ' ')
            
            source_type = "Ø§Ø¬ØªÙ‡Ø§Ø¯ Ù‚Ø¶Ø§Ø¦ÙŠ" if "Ù‚Ø±Ø§Ø±" in source_name or "Ø§Ø¬ØªÙ‡Ø§Ø¯" in source_name else "Ù†Øµ Ù‚Ø§Ù†ÙˆÙ†ÙŠ"
            # UPGRADE: No truncation for Gemini 3 (1M Context)
            # define a very large safe limit just in case
            doc_truncated = doc[:50000] 
            context += f"\n\n### [{source_type}: {source_name}]\n{doc_truncated}\n"
        
        # 5. Few-Shot Golden Example (Expanded with Eloquence)
        golden_example = """
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ“œ Ù†Ù…ÙˆØ°Ø¬ Ù…Ø±Ø§ÙØ¹Ø© Ø°Ù‡Ø¨ÙŠØ© (Ù„Ù„ØªØ¹Ù„Ù…)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
**Ø§Ù„Ù…Ø³Ø£Ù„Ø©:** Ù‡Ù„ Ø´Ù‡Ø§Ø¯Ø© Ø§Ù„Ù…Ø¬Ù†ÙŠ Ø¹Ù„ÙŠÙ‡ ÙˆØ­Ø¯Ù‡Ø§ ÙƒØ§ÙÙŠØ© Ù„Ù„Ø¥Ø¯Ø§Ù†Ø©ØŸ

**Ø§Ù„Ù‚Ø§Ø¹Ø¯Ø©:** Ù…Ù† Ø§Ù„Ù…Ø³ØªÙ‚Ø± Ø¹Ù„ÙŠÙ‡ Ù‚Ø¶Ø§Ø¡Ù‹ ÙˆÙÙ‚Ø§Ù‹ Ù„Ù„Ù…Ø§Ø¯Ø© 212 Ù…Ù† Ù‚Ø§Ù†ÙˆÙ† Ø§Ù„Ø¥Ø¬Ø±Ø§Ø¡Ø§Øª Ø§Ù„Ø¬Ø²Ø§Ø¦ÙŠØ© Ø£Ù† "Ø§Ù„Ø¥Ø«Ø¨Ø§Øª ÙÙŠ Ø§Ù„Ù…ÙˆØ§Ø¯ Ø§Ù„Ø¬Ø²Ø§Ø¦ÙŠØ© Ø­Ø±"ØŒ ØºÙŠØ± Ø£Ù† Ø§Ù„Ù…Ø­ÙƒÙ…Ø© Ø§Ù„Ø¹Ù„ÙŠØ§ Ù‚Ø±Ø±Øª ÙÙŠ Ø§Ø¬ØªÙ‡Ø§Ø¯Ù‡Ø§ Ø§Ù„Ø±Ø§Ø³Ø® Ø£Ù† "Ø§Ù„Ø´Ùƒ ÙŠÙÙØ³ÙÙ‘Ø± Ø¯Ø§Ø¦Ù…Ø§Ù‹ Ù„Ù…ØµÙ„Ø­Ø© Ø§Ù„Ù…ØªÙ‡Ù…".

**Ø§Ù„ØªØ­Ù„ÙŠÙ„:**
1. **ØªÙ†Ø§Ù‚Ø¶ ÙÙŠ Ø§Ù„ÙˆØµÙ:** Ø²Ø¹Ù… Ø§Ù„Ù…Ø¬Ù†ÙŠ Ø¹Ù„ÙŠÙ‡ Ø£Ù† Ø§Ù„Ø¬Ø§Ù†ÙŠ "Ø·ÙˆÙŠÙ„ Ø§Ù„Ù‚Ø§Ù…Ø©"ØŒ ÙÙŠ Ø­ÙŠÙ† Ø£Ù† Ø§Ù„Ù…ØªÙ‡Ù… Ù…Ø§Ø«Ù„ Ø£Ù…Ø§Ù…ÙƒÙ… ÙˆÙ‚Ø§Ù…ØªÙ‡ Ù„Ø§ ØªØªØ¬Ø§ÙˆØ² 160 Ø³Ù…ØŒ Ù…Ù…Ø§ ÙŠÙ†ÙÙŠ Ø§Ù„ØµÙ„Ø©.
2. **Ø§Ø³ØªØ­Ø§Ù„Ø© Ø§Ù„Ø­Ø¯ÙˆØ«:** Ø§Ù„ØªÙˆÙ‚ÙŠØª Ø§Ù„Ù…ØµØ±Ø­ Ø¨Ù‡ (21:00) ÙŠØªØ²Ø§Ù…Ù† Ù…Ø¹ ØªÙˆØ§Ø¬Ø¯ Ø§Ù„Ù…ØªÙ‡Ù… ÙÙŠ Ù…Ù‚Ø± Ø¹Ù…Ù„Ù‡ Ø§Ù„Ù…Ø«Ø¨Øª Ø¨ÙƒØ´Ù Ø§Ù„Ø­Ø¶ÙˆØ±.
3. **ØºÙŠØ§Ø¨ Ø§Ù„Ø³Ù†Ø¯ Ø§Ù„Ù…Ø§Ø¯ÙŠ:** Ø®Ù„Øª Ø£ÙˆØ±Ø§Ù‚ Ø§Ù„Ù…Ù„Ù Ù…Ù† Ø£ÙŠ Ù…Ø­Ø¬ÙˆØ²Ø§Øª Ø£Ùˆ Ø¨ØµÙ…Ø§Øª ØªØ¤ÙƒØ¯ Ø§Ù„Ø±ÙˆØ§ÙŠØ©.

**Ø§Ù„Ù†ØªÙŠØ¬Ø©:** ÙˆØ£Ù…Ø§Ù… Ù‡Ø°Ø§ Ø§Ù„ÙˆÙ‡Ù† ÙÙŠ Ø§Ù„Ø£Ø¯Ù„Ø©ØŒ Ù„Ø§ ÙŠØ³Ø¹ Ø¹Ø¯Ø§Ù„ØªÙƒÙ… Ø¥Ù„Ø§ Ø§Ù„Ù‚Ø¶Ø§Ø¡ Ø¨Ø§Ù„Ø¨Ø±Ø§Ø¡Ø©ØŒ ØªØ£Ø³ÙŠØ³Ø§Ù‹ Ø¹Ù„Ù‰ Ø£Ù† Ø§Ù„Ø£Ø­ÙƒØ§Ù… Ø§Ù„Ø¬Ø²Ø§Ø¦ÙŠØ© ØªÙØ¨Ù†Ù‰ Ø¹Ù„Ù‰ Ø§Ù„Ø¬Ø²Ù… ÙˆØ§Ù„ÙŠÙ‚ÙŠÙ† Ù„Ø§ Ø¹Ù„Ù‰ Ø§Ù„Ø´Ùƒ ÙˆØ§Ù„ØªØ®Ù…ÙŠÙ†.
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
"""
    
        # 6. Professional Prompt with Few-Shot + Case Data
        prompt = f"""Ø£Ù†Øª Ù…Ø­Ø§Ù…Ù Ø¬Ø²Ø§Ø¦Ø±ÙŠ "Ù†Ø§Ø¨Øº" (Top-Tier Lawyer) ØªØªØ±Ø§ÙØ¹ Ø£Ù…Ø§Ù… **{court}**.
Ù…Ù‡Ù…ØªÙƒ: ØµÙŠØ§ØºØ© **{pleading_type}** Ø¨Ø£Ø³Ù„ÙˆØ¨ Ù‚Ø§Ù†ÙˆÙ†ÙŠ Ø±ÙÙŠØ¹ØŒ ÙŠØ­Ø§ÙƒÙŠ Ø¨Ù„Ø§ØºØ© ÙƒØ¨Ø§Ø± Ø§Ù„Ù…Ø­Ø§Ù…ÙŠÙ†.

{golden_example}

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ“‹ Ù…Ù„Ù Ø§Ù„Ù‚Ø¶ÙŠØ©
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
â€¢ **Ø§Ù„Ø¬Ù‡Ø© Ø§Ù„Ù‚Ø¶Ø§Ø¦ÙŠØ©:** {court}
â€¢ **Ø§Ù„Ù…ØªÙ‡Ù…:** {defendant_name}
â€¢ **Ø§Ù„ØªÙ‡Ù…Ø©:** {charges}
{"â€¢ **Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ© Ø§Ù„Ø¯ÙØ§Ø¹:** " + main_defense if main_defense else ""}

ğŸ“ **Ø§Ù„ÙˆÙ‚Ø§Ø¦Ø¹ (Fact Pattern):**
{facts}

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ“š Ø§Ù„Ø°Ø®ÙŠØ±Ø© Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© (Ø§Ù„Ø³Ù†Ø¯)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
{context}

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
âš–ï¸ Ø§Ù„Ù‡ÙŠÙƒÙ„Ø© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø© (Ø¥Ù„Ø²Ø§Ù…ÙŠØ© Ø§Ù„Ø¹Ù†Ø§ÙˆÙŠÙ†)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
1. **Ø§Ù„Ø¯ÙŠØ¨Ø§Ø¬Ø©:** (Ø¥Ù„Ù‰ Ø§Ù„Ø³ÙŠØ¯ Ø±Ø¦ÙŠØ³ {court}...)
2. **Ø£ÙˆÙ„Ø§Ù‹: Ø§Ù„ÙˆÙ‚Ø§Ø¦Ø¹:** (Ø³Ø±Ø¯ Ù…ÙˆØ¬Ø² ÙˆÙ…Ø±Ù‚Ù…).
3. **Ø«Ø§Ù†ÙŠØ§Ù‹: Ø§Ù„Ø¥Ø¬Ø±Ø§Ø¡Ø§Øª:** (ÙÙ‚Ø±Ø© ÙˆØ§Ø­Ø¯Ø©).
4. **Ø«Ø§Ù„Ø«Ø§Ù‹: Ø§Ù„Ù…Ù†Ø§Ù‚Ø´Ø© Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© (Ø§Ù„Ù‚Ù„Ø¨):**
   - Ø§Ø³ØªØ®Ø¯Ù… Ø¹Ù†ÙˆØ§Ù† ÙØ±Ø¹ÙŠ: `### 1. ÙÙŠ Ø§Ù„Ø´ÙƒÙ„`
   - Ø§Ø³ØªØ®Ø¯Ù… Ø¹Ù†ÙˆØ§Ù† ÙØ±Ø¹ÙŠ: `### 2. ÙÙŠ Ø§Ù„Ù…ÙˆØ¶ÙˆØ¹`
   - Ø¯Ø§Ø®Ù„ Ø§Ù„Ù…ÙˆØ¶ÙˆØ¹ØŒ Ø§Ø³ØªØ®Ø¯Ù… Ù†Ù‚Ø§Ø· ÙˆØ§Ø¶Ø­Ø© `-` Ù„ÙƒÙ„ Ø¯ÙØ¹ Ù‚Ø§Ù†ÙˆÙ†ÙŠ.
   - Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø¨Ù†ÙŠØ©: **Ø§Ù„Ø¯ÙÙ€Ø¹** â† **Ø§Ù„Ø³Ù†Ø¯ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠ (Ø§Ù„Ù…Ø§Ø¯Ø©)** â† **Ø§Ù„ØªØ·Ø¨ÙŠÙ‚ Ø¹Ù„Ù‰ Ø§Ù„ÙˆÙ‚Ø§Ø¦Ø¹**.
5. **Ø±Ø§Ø¨Ø¹Ø§Ù‹: Ø§Ù„Ø·Ù„Ø¨Ø§Øª:** (Ù‚Ø§Ø¦Ù…Ø© Ù†Ù‚Ø·ÙŠØ© Ù…Ø±Ù‚Ù…Ø©).

âš ï¸ **ØªÙ†Ø¨ÙŠÙ‡Ø§Øª**:
- **Ø§Ù„ØªÙ†Ø³ÙŠÙ‚:** Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø¹Ù†Ø§ÙˆÙŠÙ† `###` ÙˆØ§Ù„Ù†Ù‚Ø§Ø· `-` Ù„ØªØ¬Ù…ÙŠÙ„ Ø§Ù„Ù†Øµ ÙˆØ¬Ø¹Ù„Ù‡ Ù…Ù‚Ø±ÙˆØ¡Ø§Ù‹.
- **Ø§Ù„Ø§Ù‚ØªØ¨Ø§Ø³:** Ø¶Ø¹ Ù†ØµÙˆØµ Ø§Ù„Ù…ÙˆØ§Ø¯ Ø¨ÙŠÙ† Â«...Â».

Ø§Ø¨Ø¯Ø£ Ø§Ù„Ù…Ø±Ø§ÙØ¹Ø© ÙÙˆØ±Ø§Ù‹:"""

        try:
            print(f"[Pleading] Sending prompt of length: {len(prompt)} chars")
            # USE DEDICATED OPENROUTER FUNCTION
            response = generate_openrouter(prompt)
            pleading_text = response.text
            
            # --- POST-PROCESSING (Cleaning) ---
            # 1. Clean Cyrillic/Russian OCR artifacts (common in scraped Algerian laws)
            pleading_text = re.sub(r'[\u0400-\u04FF]+', '', pleading_text)
            # 2. Remove any remaining bracketed placeholders if LLM hallucinated them
            pleading_text = re.sub(r'\[(?!Ù…ØµØ¯Ø±|Ù†Øµ|Ø§Ø¬ØªÙ‡Ø§Ø¯).*?\]', '', pleading_text)
            
            print(f"[Pleading] SUCCESS - Generated {len(pleading_text)} chars")
        except Exception as e:
            print(f"[Pleading] FAILED: {type(e).__name__}: {e}")
            pleading_text = f"""# Ù…Ø°ÙƒØ±Ø© {pleading_type}

âš ï¸ Ø¹Ø°Ø±Ø§Ù‹ØŒ Ù„Ù… Ø£ØªÙ…ÙƒÙ† Ù…Ù† Ø¥ØªÙ…Ø§Ù… ØµÙŠØ§ØºØ© Ø§Ù„Ù…Ø°ÙƒØ±Ø© Ø¨Ø³Ø¨Ø¨ Ø®Ø·Ø£ ØªÙ‚Ù†ÙŠ.
**Ø§Ù„Ø®Ø·Ø£:** {type(e).__name__}: {str(e)[:200]}

## Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù…ØªØ§Ø­Ø©:
- **Ø§Ù„Ù…ØªÙ‡Ù…**: {defendant_name}
- **Ø§Ù„ØªÙ‡Ù…Ø©**: {charges}

ÙŠØ±Ø¬Ù‰ Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø©."""


        # Build sources with document_id and chunk_index for interactivity
        sources_list = []
        for d, m in zip(final_docs, final_metas):
            title = m.get('filename', 'Source').replace('.txt', '')
            title = re.sub(r'[\u0400-\u04FF]+', 'Ø¹Ù„Ù‰', title)
            title = title.replace(' Ğ½Ğ° ', ' Ø¹Ù„Ù‰ ').replace(' ha ', ' Ø¹Ù„Ù‰ ')
            title = title.replace('_', ' ')
            sources_list.append({
                "title": title,
                "filename": m.get('filename'),
                "document_id": m.get("document_id"),
                "chunk_index": m.get("chunk_index")
            })

        return {
            "pleading": pleading_text,
            "metadata": {"total_sources": len(docs), "pleading_type": pleading_type},
            "sources": sources_list
        }


    def search_jurisprudence(self, legal_issue: str, chamber=None, top_k=20):
        # Jurisprudence Mode - Filter by Supreme Court and Conseil d'Ã‰tat
        
        # If chamber is specified, append it to query
        search_query = legal_issue
        if chamber:
             search_query += f" ({chamber})"
             
        # FIX: Include all jurisprudence categories (database uses 'jurisprudence_full')
        target_categories = ["jurisprudence", "jurisprudence_full", "jurisprudence_conseil_etat"]
        
        # UPGRADE: Fetch broad (200) then strictly filter in Python 
        # (RPC doesn't support $in queries, so we fetch more to be safe)
        raw_docs, raw_metas = self._retrieve(search_query, filters=None, top_k=200)
        
        docs = []
        metas = []
        
        for d, m in zip(raw_docs, raw_metas):
            if m.get("category") in target_categories:
                docs.append(d)
                metas.append(m)
        
        # Debug: Log how many jurisprudence docs were found
        print(f"[Jurisprudence] Found {len(docs)} matching documents out of {len(raw_docs)} total retrieved")
        
        # Slice to requested top_k
        docs = docs[:top_k]
        metas = metas[:top_k]
        
        # If no jurisprudence docs found, inform the user clearly
        if len(docs) == 0:
            return {
                "analysis": "âš ï¸ Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø§Ø¬ØªÙ‡Ø§Ø¯Ø§Øª Ù‚Ø¶Ø§Ø¦ÙŠØ© Ù…Ø·Ø§Ø¨Ù‚Ø© Ù„Ù„Ù…Ø³Ø£Ù„Ø© Ø§Ù„Ù…Ø·Ø±ÙˆØ­Ø© ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø­Ø§Ù„ÙŠØ©. ÙŠÙØ±Ø¬Ù‰ ØªØ¬Ø±Ø¨Ø© ØµÙŠØ§ØºØ© Ø£Ø®Ø±Ù‰ Ù„Ù„Ø³Ø¤Ø§Ù„ Ø£Ùˆ Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø¥Ø¯Ø®Ø§Ù„ Ø§Ù„Ø§Ø¬ØªÙ‡Ø§Ø¯Ø§Øª.",
                "metadata": {"total_sources": 0},
                "sources": []
            }
        
        # RERANKING: Use Gemini to filter only jurisprudence relevant to the legal issue
        # This prevents unrelated cases (e.g., property law when searching for confession validity)
        try:
            print(f"[Jurisprudence] Reranking {len(docs)} documents for relevance...")
            # UPGRADE: Rerank more docs for Gemini 3
            reranked = rerank_with_gemini(legal_issue, docs, top_k=20)
            
            # Rebuild docs/metas based on reranked order
            reranked_docs = [r[0] for r in reranked]
            doc_to_meta = {d: m for d, m in zip(docs, metas)}
            reranked_metas = [doc_to_meta.get(d, {}) for d in reranked_docs]
            
            docs = reranked_docs
            metas = reranked_metas
            print(f"[Jurisprudence] After reranking: {len(docs)} documents retained")
        except Exception as e:
            print(f"[Jurisprudence] Reranking failed: {e}, using original order")
            # Fallback: just take top 5
        # UPGRADE: Use more docs and no truncation for Gemini 3
            docs = docs[:20]
            metas = metas[:20]
        
        # Limit context to avoid token limit - REMOVED for Gemini 3
        # We pass full content now
        context = "\n".join([f"--- Ù‚Ø±Ø§Ø± {i+1} ({metas[i].get('filename', 'ØºÙŠØ± Ù…Ø¹Ø±ÙˆÙ')}) ---\n{d}" for i, d in enumerate(docs)])
        
        prompt = f"""Ø¨ØµÙØªÙƒ Ø¨Ø§Ø­Ø«Ø§Ù‹ ÙÙŠ Ø§Ù„Ø§Ø¬ØªÙ‡Ø§Ø¯ Ø§Ù„Ù‚Ø¶Ø§Ø¦ÙŠ (Ø§Ù„Ù…Ø­ÙƒÙ…Ø© Ø§Ù„Ø¹Ù„ÙŠØ§ ÙˆÙ…Ø¬Ù„Ø³ Ø§Ù„Ø¯ÙˆÙ„Ø©).
Ø§Ù„Ù…Ø³Ø£Ù„Ø©: {legal_issue}
Ø§Ù„Ù‚Ø±Ø§Ø±Ø§Øª Ø§Ù„Ù…Ø³ØªØ®Ø±Ø¬Ø©:
{context}

Ø§Ù„Ù…Ø·Ù„ÙˆØ¨:
1. Ø§Ø³ØªØ®Ø±Ø¬ Ø§Ù„Ù…Ø¨Ø§Ø¯Ø¦ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© Ø¨Ø¯Ù‚Ø© Ù…Ù† Ø§Ù„Ù‚Ø±Ø§Ø±Ø§Øª Ø£Ø¹Ù„Ø§Ù‡ ÙÙ‚Ø·.
2. Ù„ÙƒÙ„ Ù…Ø¨Ø¯Ø£ØŒ **ÙŠØ¬Ø¨** Ø¥Ø¯Ø±Ø§Ø¬ "Ù†Øµ Ø§Ù„Ù…Ø¨Ø¯Ø£" ÙƒÙ…Ø§ ÙˆØ±Ø¯ ÙÙŠ Ø§Ù„Ù‚Ø±Ø§Ø± Ø¨ÙŠÙ† Ø¹Ù„Ø§Ù…ØªÙŠ Ø§Ù‚ØªØ¨Ø§Ø³.
3. Ø§Ø°ÙƒØ± Ø±Ù‚Ù… Ø§Ù„Ù‚Ø±Ø§Ø± ÙˆØªØ§Ø±ÙŠØ®Ù‡ Ø¥Ù† ÙˆØ¬Ø¯ ÙÙŠ Ø§Ù„Ù†ØµØŒ **Ø£Ùˆ Ø§ÙƒØªØ¨ "ØºÙŠØ± Ù…Ø°ÙƒÙˆØ±"**.
4. ÙˆØ¶Ø­ Ù‡Ù„ Ø§Ù„Ø§Ø¬ØªÙ‡Ø§Ø¯ Ù…Ø³ØªÙ‚Ø± Ø£Ù… Ù‡Ù†Ø§Ùƒ ØªÙ†Ø§Ù‚Ø¶.

âš ï¸ **ØªØ­Ø°ÙŠØ± Ù…Ù‡Ù…**: Ù„Ø§ ØªØ®ØªÙ„Ù‚ Ø£Ø±Ù‚Ø§Ù… Ù‚Ø±Ø§Ø±Ø§Øª Ø£Ùˆ ØªÙˆØ§Ø±ÙŠØ® Ø£Ùˆ Ù…Ø±Ø§Ø¬Ø¹ ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯Ø© ÙÙŠ Ø§Ù„Ù†ØµÙˆØµ Ø£Ø¹Ù„Ø§Ù‡. Ø¥Ø°Ø§ Ù„Ù… ØªØ¬Ø¯ Ø±Ù‚Ù… Ø§Ù„Ù‚Ø±Ø§Ø±ØŒ Ø§ÙƒØªØ¨ ØµØ±Ø§Ø­Ø©: "Ø±Ù‚Ù… Ø§Ù„Ù‚Ø±Ø§Ø±: ØºÙŠØ± Ù…Ø°ÙƒÙˆØ± ÙÙŠ Ø§Ù„Ù†Øµ".

Ø§Ù„ØªÙ†Ø³ÙŠÙ‚ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨:
- **Ø§Ù„Ù…Ø¨Ø¯Ø£:** [Ø´Ø±Ø­ Ø§Ù„Ù…Ø¨Ø¯Ø£]
- **Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ù‚ØªØ¨Ø³:** "...[Ø§Ù„Ù†Øµ Ø§Ù„Ø­Ø±ÙÙŠ Ù…Ù† Ø§Ù„Ù‚Ø±Ø§Ø±]..."
- **Ø§Ù„Ù…Ø±Ø¬Ø¹:** Ù‚Ø±Ø§Ø± Ø±Ù‚Ù… [X] Ø¨ØªØ§Ø±ÙŠØ® [Y] - [Ø§Ù„Ø¬Ù‡Ø©] (Ø£Ùˆ "ØºÙŠØ± Ù…Ø°ÙƒÙˆØ±")"""

        # UPGRADE: Use OpenRouter for better Arabic legal understanding
        response = generate_openrouter(prompt)
        
        # Include text snippets in sources for UI
        enriched_sources = []
        for doc, meta in zip(docs, metas): # Ensure we return all reranked docs
             enriched_sources.append({
                 "filename": meta.get('filename'),
                 "document_id": meta.get('document_id'),
                 "chunk_index": meta.get('chunk_index', 1),
                 "relevance_score": 0.9,
                 "snippet": doc[:200] + "..." # Snippet for UI
             })

        return {
            "analysis": response.text,
            "metadata": {"total_sources": len(docs)},
            "sources": enriched_sources
        }

rag_service = RAGService()

def rag_pipeline(query, filters=None, skip_generation=False):
    return rag_service.answer_query(query, filters, skip_generation)
